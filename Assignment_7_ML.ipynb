{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e543345d",
   "metadata": {},
   "source": [
    "### 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function&#39;s fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed30297",
   "metadata": {},
   "source": [
    "A target function, also known as an objective function or cost function, is a mathematical representation that measures how well a particular solution or model performs with respect to the desired outcome. The target function provides a way to quantify the quality of a solution or model by assigning a numerical value, often called the fitness or cost, based on its performance.\n",
    "\n",
    "To provide a real-life example, let's consider the problem of predicting housing prices. Suppose you have a dataset containing information about different houses, such as their size, number of bedrooms, location, and other relevant features, along with their corresponding sale prices. The task is to build a model that can accurately predict the sale price of a house given its features.\n",
    "\n",
    "In this scenario, the target function could be the mean squared error (MSE) between the predicted sale prices by the model and the actual sale prices in the dataset. The MSE is a common choice for regression problems and is calculated by taking the average of the squared differences between the predicted and actual values. \n",
    "\n",
    "Target function = Mean Squared Error (MSE)\n",
    "\n",
    "Fitness or cost = Average of (predicted sale price - actual sale price)^2\n",
    "\n",
    "During the training process, the model iteratively adjusts its parameters to minimize the target function. By minimizing the MSE, the model aims to find the best possible fit to the training data, leading to accurate predictions for new, unseen data.\n",
    "To assess the fitness, we calculate the MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6dc1c",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef5f9e",
   "metadata": {},
   "source": [
    "### 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdabeb4",
   "metadata": {},
   "source": [
    "Descriptive models aim to understand and describe the data, while predictive models aim to make accurate predictions about future outcomes.\n",
    "Descriptive models focus on summarizing and explaining historical data or observations, whereas predictive models focus on learning patterns from historical data to make predictions about future events.\n",
    "Descriptive models typically use statistical and exploratory techniques, whereas predictive models often utilize machine learning algorithms and statistical modeling techniques.\n",
    "Descriptive models analyze past data, while predictive models make projections into the future.\n",
    "Descriptive models are evaluated based on the goodness of fit, descriptive statistics, and explanatory power. Predictive models are evaluated based on their ability to make accurate predictions using metrics such as accuracy, precision, recall, and error measures (e.g., MSE, RMSE).\n",
    "\n",
    "E.g. of descriptive models is factors that influence customer satisfaction based on historical data. To build a descriptive model, you could use statistical techniques such as regression analysis or data visualization methods to explore the relationships between different variables and customer satisfaction.\n",
    "E.g. of predictive model would be to assess the creditworthiness of new customers and make informed decisions, such as approving or denying credit applications, setting credit limits, or adjusting interest rates. So based on historical data one can predict potenital risks etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197015a",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9694ba",
   "metadata": {},
   "source": [
    "### 3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various measurement parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d64aa8",
   "metadata": {},
   "source": [
    "Assessing the efficiency of a classification model involves evaluating its performance in correctly classifying instances into different categories or classes. There are several measurement parameters commonly used to assess the performance of classification models.\n",
    "Some of those are:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 score\n",
    "5. Speicificity\n",
    "6. Area under the ROC\n",
    "7. Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f9fe1",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ee848",
   "metadata": {},
   "source": [
    "### 4.\n",
    "i. In the sense of machine learning models, what is underfitting? What is the most common\n",
    "reason for underfitting?\n",
    "\n",
    "Underfitting refers to a situation where a model is too simple or lacks the capacity to capture the underlying patterns and relationships in the data. Characteristics:The model may have high bias and low variance. It fails to capture the complexities of the data and performs poorly on both the training and testing/validation sets. It often exhibits a high error rate and fails to generalize well to unseen data. Causes: Using an overly simplistic model that cannot capture the nuances of the data. Insufficient training or lack of data. Inadequate feature engineering or feature selection, where important predictors are not included.\n",
    "\n",
    "\n",
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "Overfitting occurs when a model becomes too complex or is excessively tailored to the training data, capturing noise or random fluctuations that are not representative of the true underlying patterns.\n",
    "Characteristics:The model has low bias and high variance. It performs exceptionally well on the training data but fails to generalize to new, unseen data. It often exhibits a significant gap between the training and testing/validation performance, indicating poor generalization. Causes: Using a complex model with a large number of parameters relative to the available data. Including irrelevant or noisy features that lead to overemphasis on random fluctuations. Insufficient regularization or lack of proper techniques to control overfitting.\n",
    "\n",
    "\n",
    "iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
    "The bias-variance trade-off arises because reducing bias often increases variance, and reducing variance often increases bias. It's a balancing act between finding the right level of model complexity that minimizes both bias and variance to achieve good generalization performance.\n",
    "The goal is to strike a balance between bias and variance by finding an optimal level of model complexity that minimizes both errors. This is typically achieved through techniques such as cross-validation, regularization, or model selection algorithms. The trade-off is about finding the right level of complexity that allows the model to generalize well and perform effectively on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99356344",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b8016",
   "metadata": {},
   "source": [
    "### 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877e079",
   "metadata": {},
   "source": [
    "### Yes, it is possible to boost the efficiency of a learning model. By applying strategies such as data preprocessing, feature selection, algorithm selection, model architecture optimization, hyperparameter tuning, parallel computing, transfer learning and model ensembling you can boost the efficiency of a learning model and achieve better performance in terms of training time, computational resources, and overall effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d11172",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3f9d0",
   "metadata": {},
   "source": [
    "### 6. How would you rate an unsupervised learning model&#39;s success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352aa2d",
   "metadata": {},
   "source": [
    "### The evaluation and rating of an unsupervised learning model's success can be challenging because unsupervised learning algorithms do not have access to explicit labels or ground truth for comparison.\n",
    "\n",
    "### Nevertheless, there are several common success indicators used to evaluate the performance of unsupervised learning models:\n",
    "\n",
    "1. Silhouette score\n",
    "2. Calinski-Harabasz index\n",
    "3. Visualization such as PCA or t-SNE.\n",
    "4. Reconstruction error - MSE, RMSE\n",
    "5. Dunn index\n",
    "6. Xie-Beni score\n",
    "7. Harigan index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8dc587",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da4379",
   "metadata": {},
   "source": [
    "### 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64cfc7",
   "metadata": {},
   "source": [
    "### Using a classification model for numerical data or a regression model for categorical data can lead to problems because the models are trained with assumptions that may not hold true for the data at hand. But if you need to convert classification data to numerical then use one hot encoder or label encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa2433",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f81d3",
   "metadata": {},
   "source": [
    "### 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4865157",
   "metadata": {},
   "source": [
    "### The predictive modeling method for numerical values, also known as regression modeling, aims to predict a continuous numeric target variable based on a set of input features. It involves developing a mathematical relationship between the input variables and the target variable to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2db4fc",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da5f87",
   "metadata": {},
   "source": [
    "### 9. The following data were collected when using a classification model to predict the malignancy of a group of patients&#39; tumors:\n",
    "- i. Accurate estimates – 15 cancerous, 75 benign\n",
    "- ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "- Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636415f",
   "metadata": {},
   "source": [
    "Error Rate:\n",
    "The error rate represents the proportion of incorrect predictions made by the model.\n",
    "Total predictions = 15 (cancerous) + 75 (benign) + 3 (wrong cancerous) + 7 (wrong benign) = 100\n",
    "Error rate = (Wrong predictions / Total predictions) = (3 + 7) / 100 = 0.1 or 10%\n",
    "\n",
    "Kappa Value:\n",
    "The Kappa value measures the agreement between the predicted and actual classifications, considering the agreement that could be expected by chance.\n",
    "Let's calculate the observed agreement and expected agreement:\n",
    "Observed agreement = (Correctly predicted cancerous + Correctly predicted benign) / Total predictions\n",
    "= (15 + 75) / 100 = 0.9 or 90%\n",
    "\n",
    "Expected agreement = [(Total predicted cancerous / Total predictions) * (Total actual cancerous / Total predictions)]\n",
    "+ [(Total predicted benign / Total predictions) * (Total actual benign / Total predictions)]\n",
    "= [(15 + 3) / 100) * ((15 + 75) / 100) + [(75 + 7) / 100) * ((75 + 75) / 100)\n",
    "= (0.18 * 0.9) + (0.82 * 0.9) = 0.162 + 0.738 = 0.9 or 90%\n",
    "\n",
    "Kappa value = (Observed agreement - Expected agreement) / (1 - Expected agreement)\n",
    "= (0.9 - 0.9) / (1 - 0.9) = 0\n",
    "\n",
    "Sensitivity (True Positive Rate or Recall):\n",
    "Sensitivity measures the proportion of actual cancerous tumors correctly identified by the model.\n",
    "Sensitivity = (Correctly predicted cancerous / Total actual cancerous) = 15 / (15 + 3) = 0.8333 or 83.33%\n",
    "\n",
    "Precision:\n",
    "Precision measures the proportion of predicted cancerous tumors that are actually cancerous.\n",
    "Precision = (Correctly predicted cancerous / Total predicted cancerous) = 15 / (15 + 7) = 0.6818 or 68.18%\n",
    "\n",
    "F-measure:\n",
    "The F-measure combines precision and sensitivity into a single metric to evaluate the overall performance of the classification model.\n",
    "F-measure = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n",
    "= 2 * (0.6818 * 0.8333) / (0.6818 + 0.8333) = 0.7517 or 75.17%\n",
    "\n",
    "So, based on the given data, the model's metrics are as follows:\n",
    "\n",
    "- Error rate: 10%\n",
    "- Kappa value: 0\n",
    "- Sensitivity: 83.33%\n",
    "- Precision: 68.18%\n",
    "- F-measure: 75.17%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0f584b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, precision_score\n",
    "import pandas as pd\n",
    "\n",
    "y_true = pd.DataFrame([15,75])\n",
    "y_pred = pd.DataFrame([3,7])\n",
    "\n",
    "kappa = cohen_kappa_score(y_true,y_pred)\n",
    "print (kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b99d6f",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a17ed",
   "metadata": {},
   "source": [
    "### 10. Make quick notes on:\n",
    "1. The process of holding out\n",
    "\n",
    "- Holding out, also known as the holdout method, is a common practice in machine learning and predictive modeling. It involves reserving a portion of the available dataset as a validation or test set and not using it during the training phase. The holdout set is used to evaluate the model's performance and assess its generalization ability on unseen data. \n",
    "\n",
    "\n",
    "2. Cross-validation by tenfold \n",
    "\n",
    "- The dataset is divided into 10 approximately equal-sized subsets or folds. Each fold contains an equal proportion of samples from the dataset. One fold is held out as the validation set, while the remaining nine folds are used as the training set.The model is trained on the 9 folds of training set and the reserved fold is used for validation. The process is repeated 10 times so each time the validation fold is a new training set. \n",
    "\n",
    "3. Adjusting the parameters\n",
    "\n",
    "-  training the set of parameter values which are optimal in some desired sense. They are usually the weights and biases of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070279d",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b93c07",
   "metadata": {},
   "source": [
    "### 11. Define the following terms:\n",
    "1. Purity vs. Silhouette width\n",
    "2. Boosting vs. Bagging\n",
    "3. The eager learner vs. the lazy learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f323c",
   "metadata": {},
   "source": [
    "### Purity - Purity is a metric used in machine learning to evaluate the quality of clustering algorithms, particularly in unsupervised learning tasks. It measures how well the clusters generated by the algorithm align with the ground truth or known labels of the data.\n",
    "\n",
    "### Purity is calculated by assigning each cluster to the majority class of the data points it contains and then summing up the proportions of correctly assigned data points across all clusters.\n",
    "### Purity = (1 / N) * Σ(maximum count in cluster k), where N is the total number of data points and the summation is taken over all clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bee297",
   "metadata": {},
   "source": [
    "### Silhouette width -  an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833967a4",
   "metadata": {},
   "source": [
    "### Boosting - combines multiple weak or base models to create a strong predictive model. It aims to improve the performance of individual models by sequentially building a series of models that focus on the previously misclassified or difficult examples. The basic idea behind boosting is to iteratively train weak models, often referred to as \"weak learners,\" and give more weight or emphasis to the misclassified instances during each iteration. The weak learners are typically simple models, such as decision trees with limited depth or simple rules.\n",
    "\n",
    "### Bagging - Bagging, short for bootstrap aggregating, is an ensemble machine learning technique that combines the predictions of multiple base models to make a final prediction. It aims to reduce the variance and improve the overall performance and robustness of the individual models by leveraging the principle of bootstrap sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61955fa3",
   "metadata": {},
   "source": [
    "### Eager learner -  is a type of machine learning algorithm that eagerly builds a generalization model from the training data before receiving any new, unseen data for prediction. Eager learners construct their models during the training phase and use this pre-built model to make predictions on new instances directly without further modifications.\n",
    "### Lazy learner - where the model postpones the learning process until it receives a specific prediction request for a new data point. Lazy learners store the training data and make predictions on new instances by comparing them to the stored training instances at prediction time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
