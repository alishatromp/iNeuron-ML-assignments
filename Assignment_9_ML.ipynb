{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70959402",
   "metadata": {},
   "source": [
    "### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e85cb6",
   "metadata": {},
   "source": [
    "### Feature engineering is the process of transforming raw data into a format that is suitable for machine learning algorithms. It involves selecting, creating, and transforming features (input variables) to improve the performance of a machine learning model.\n",
    "\n",
    "Various aspects of feature engineering are:\n",
    "1. Data collection\n",
    "2. Data cleaning\n",
    "3. Feature selection\n",
    "4. Feature creation\n",
    "5. Feature transformation\n",
    "6. Feature encoding\n",
    "7. Feature scaling\n",
    "8. Iteration\n",
    "9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94476839",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8378a",
   "metadata": {},
   "source": [
    "### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd1087",
   "metadata": {},
   "source": [
    "In Feature Selection we identify the most relevant features for your machine learning task. This involves analyzing the data and selecting the subset of features that have the most predictive power. \n",
    "Removing irrelevant or redundant features can reduce noise and improve model performance.\n",
    "\n",
    "1. Univariate selection\n",
    "2. Recursive feature elimination\n",
    "3. L1 regularization (Lasso) and L2 (Ridge)\n",
    "4. Tree based methods\n",
    "5. PCA\n",
    "6. Sequential feature selection\n",
    "7. Correlation based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d6710",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699da715",
   "metadata": {},
   "source": [
    "### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56482bf",
   "metadata": {},
   "source": [
    "### Function Selection:\n",
    "Function selection methods evaluate the relevance of features based on some statistical measure or scoring function without involving the actual machine learning model. Some techniques include:\n",
    "- Information Gain\n",
    "- Chi-square test\n",
    "- Mutual information\n",
    "\n",
    "### Wrapper methods:\n",
    "Wrapper methods involve using a machine learning model to evaluate subsets of features based on their impact on model performance. These methods incorporate a \"wrapper\" around the model and perform a search over different feature subsets. The performance of the model is used as a feedback mechanism to guide the search process.\n",
    "- RFE\n",
    "- SFS\n",
    "- Genetic algorithms\n",
    "\n",
    "### Pros:\n",
    "wrapper involves evaluating multiple models with different subsets.\n",
    "provides more accurate results based on models performance.\n",
    "FS - are faster and ranks features based on relevance\n",
    "wrapper considers feature interactions.\n",
    "\n",
    "### Cons:\n",
    "wrapper is computationally expensive\n",
    "Fs - doesn't consider interactions between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca041f",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8c192",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "### i. Describe the overall feature selection process.\n",
    "\n",
    "- Feature selection is the process of choosing a subset of relevant features from the original set of features. It helps to improve model performance, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "1. Define the Problem: Clearly define the problem you are trying to solve and determine the goal of your machine learning model. This helps in understanding the type of features that might be relevant for the task.\n",
    "\n",
    "2. Data Collection and Preparation: Gather the dataset that contains the features and the corresponding target variable. Preprocess the data by handling missing values, outliers, and data inconsistencies. Split the dataset into training and testing sets to ensure unbiased evaluation.\n",
    "\n",
    "3. Explore the Data: Perform exploratory data analysis to gain insights into the distribution, relationships, and characteristics of the features. Identify any obvious patterns, correlations, or outliers that might impact the feature selection process.\n",
    "\n",
    "4. Select Feature Selection Method: Choose the appropriate feature selection method(s) based on the problem type, available data, and computational resources. Consider whether function selection methods, wrapper methods, or a combination of both would be suitable.\n",
    "\n",
    "5. Apply Feature Selection: Implement the selected feature selection method(s) on the training data. This may involve ranking features, evaluating subsets of features, or applying scoring functions to determine feature importance.\n",
    "\n",
    "6. Evaluate Model Performance: Train a machine learning model on the selected subset of features and evaluate its performance using appropriate metrics. This step helps assess the impact of feature selection on the model's accuracy, generalization, and interpretability.\n",
    "\n",
    "7. Iterate and Refine: Iterate the feature selection process by experimenting with different feature subsets, tweaking parameters, or trying alternative methods. This iterative process allows you to refine the feature selection process and improve the model's performance.\n",
    "\n",
    "8. Finalize Feature Subset: Once you are satisfied with the performance of the model using the selected features, finalize the feature subset. Ensure that the selected features are reliable, relevant, and provide meaningful insights for the problem at hand.\n",
    "\n",
    "9. Test on Unseen Data: Validate the final feature subset and the trained model on unseen data, typically the testing dataset. This helps assess the model's ability to generalize and make accurate predictions on new, unseen examples.\n",
    "\n",
    "10. Monitor and Update: Continuously monitor the performance of the model in real-world scenarios. If necessary, re-evaluate the feature selection process and update the feature subset to adapt to changes in the data or the problem requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971482a",
   "metadata": {},
   "source": [
    "### ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "\n",
    "\n",
    "The key underlying principle of feature extraction is to transform the raw data into a new representation that captures the essential information or patterns in a more compact and informative way. This process reduces the dimensionality of the data while retaining the most relevant information for the machine learning task at hand.\n",
    "\n",
    "One commonly used feature extraction algorithm for images is the Principal Component Analysis (PCA). PCA aims to find the directions of maximum variance in the data and project the data onto these directions, resulting in a new set of features called principal components.\n",
    "\n",
    "Let's apply PCA to an image processing example:\n",
    "Suppose we have a dataset of images representing handwritten digits, and our goal is to build a digit recognition model. Each image is initially represented as a grid of pixels, where each pixel's value represents its intensity or color.\n",
    "\n",
    "To apply PCA for feature extraction in this example:\n",
    "\n",
    "- First, we preprocess the images by resizing them to a fixed size and converting them to grayscale.\n",
    "- We then flatten each image into a vector, representing it as a long feature vector.\n",
    "- Next, we normalize the feature vectors to have zero mean and unit variance.\n",
    "- Now, we can apply PCA to the normalized feature vectors. PCA calculates the covariance matrix of the data and finds the eigenvectors corresponding to the largest eigenvalues.\n",
    "- The eigenvectors (principal components) capture the directions of maximum variance in the data. We can choose to keep a certain number of principal components, typically based on the amount of variance they explain.\n",
    "- Finally, we project the normalized feature vectors onto the selected principal components to obtain a reduced-dimensional representation of the images. These projected values are the extracted features that will be used for training a digit recognition model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4fd31",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f05f2",
   "metadata": {},
   "source": [
    "### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3c649",
   "metadata": {},
   "source": [
    "### In the context of text categorization, the feature engineering process involves transforming raw text data into numerical representations that can be used by machine learning algorithms to classify and categorize texts.\n",
    "\n",
    "1. Text Preprocessing:\n",
    "\n",
    "- Tokenization: Split the text into individual words or tokens.\n",
    "- Lowercasing: Convert all words to lowercase to treat words with the same characters as identical.\n",
    "- Removing Punctuation: Remove punctuation marks that don't carry significant meaning.\n",
    "- Stop Word Removal: Remove common words that occur frequently but may not contribute much to the meaning (e.g., \"the,\" \"is,\" \"and\").\n",
    "- Stemming or Lemmatization: Reduce words to their base form (stem) or their canonical form (lemma) to handle variations (e.g., \"running\" to \"run\").\n",
    "\n",
    "2. Feature Extraction:\n",
    "\n",
    "- Bag-of-Words (BoW): Create a vocabulary of unique words from the preprocessed text and represent each text document as a vector where each dimension represents the count or presence of a word in the document.\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF): Assign weights to words based on their frequency in a document and their rarity across the entire corpus, emphasizing words that are discriminative for a particular category.\n",
    "- Word Embeddings: Utilize pre-trained word embeddings like Word2Vec, GloVe, or FastText to represent words as dense vectors capturing semantic meaning. These embeddings can be averaged or concatenated to represent an entire text document.\n",
    "- N-grams: Capture the relationship between adjacent words by considering sequences of words (e.g., bigrams, trigrams) in addition to individual words.\n",
    "\n",
    "3. Feature Selection:\n",
    "\n",
    "- Remove Low-Frequency or High-Frequency Words: Exclude words that occur too infrequently or too frequently, as they may not carry significant information.\n",
    "- Select Top-K Features: Based on statistical measures like information gain, chi-square, or mutual information, select the most informative features that are relevant for categorization.\n",
    "\n",
    "4. Encoding and Vectorization:\n",
    "\n",
    "- Convert Categorical Features: Encode categorical features like author names, document sources, or genres into numerical representations using techniques like one-hot encoding or label encoding.\n",
    "- Vectorization: Transform the text features into numerical vectors that can be consumed by machine learning algorithms. This could involve concatenating different feature representations or using specific vectorization methods like TF-IDF vectorization.\n",
    "\n",
    "5. Model-specific Transformations:\n",
    "\n",
    "- Depending on the model being used, further transformations may be required. For example, for deep learning models, you might need to pad sequences to a fixed length or convert text to sequences of integer indices.\n",
    "\n",
    "6. Iteration and Evaluation:\n",
    "\n",
    "- Experiment with different feature engineering techniques, such as different text preprocessing steps, feature extraction methods, or feature selection strategies.\n",
    "- Evaluate the performance of the model using appropriate evaluation metrics and iterate on the feature engineering process until satisfactory results are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522875",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff187f",
   "metadata": {},
   "source": [
    "### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3bc4b",
   "metadata": {},
   "source": [
    "1. Dimensionality Independence: Cosine similarity measures the cosine of the angle between two vectors, regardless of the vector's magnitude or length. In text categorization, documents are represented as vectors in a high-dimensional space (e.g., document-term matrix), where each dimension represents a unique term. Cosine similarity allows us to focus on the direction of the vectors rather than their magnitude, making it effective for comparing and categorizing documents regardless of their length.\n",
    "\n",
    "2. Insensitivity to Document Length: Cosine similarity is not affected by the length of the documents being compared. Documents with varying lengths can still have similar cosine similarity scores if they have similar term distributions. This is beneficial in text categorization tasks where document lengths can vary significantly.\n",
    "\n",
    "3. Semantic Similarity: Cosine similarity captures the similarity of the document's content rather than the actual term frequency values. It considers the relative positions of the vectors in the high-dimensional space, which can reflect the semantic similarity between documents. Even if two documents use different term frequencies, they can still have a high cosine similarity if they have similar term patterns and distributions.\n",
    "\n",
    "To calculate the cosine similarity, we use the formula:\n",
    "\n",
    "cosine_similarity = dot_product(A, B) / (norm(A) * norm(B))\n",
    "\n",
    "Vector A = (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
    "Vector B = (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
    "\n",
    "dot_product(A,B) = dot_product(A) x dot_product(B)\n",
    "norm(A) and norm(B) are the Euclidean norms of vector A and B.\n",
    "\n",
    "dot_product(A, B) = (22) + (31) + (20) + (00) + (23) + (32) + (31) + (03) + (1*1) = 23\n",
    "\n",
    "norm(A) = sqrt((2^2) + (3^2) + (2^2) + (0^2) + (2^2) + (3^2) + (3^2) + (0^2) + (1^2)) = sqrt(46)\n",
    "\n",
    "norm(B) = sqrt((2^2) + (1^2) + (0^2) + (0^2) + (3^2) + (2^2) + (1^2) + (3^2) + (1^2)) = sqrt(24)\n",
    "\n",
    "cosine_similarity = 23 / (sqrt(46) * sqrt(24)) = 0.72\n",
    "\n",
    "Therefore resemblance is 72%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecec2d",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0d8ce",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "### i. What is the formula for calculating Hamming distance? \n",
    "\n",
    "### The Hamming distance is a measure of the difference or gap between two strings of equal length. It counts the number of positions at which the corresponding elements in the two strings are different\n",
    "### - Hamming_distance = number of positions with differing elements\n",
    "\n",
    "### Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "String 1: 10001011\n",
    "\n",
    "String 2: 11001111\n",
    "\n",
    "- The first element in both strings is different: \"1\" vs. \"1\" -> 0 difference\n",
    "- The second element in both strings is the same: \"0\" vs. \"1\" -> 1 difference\n",
    "- The third element in both strings is the same: \"0\" vs. \"0\" -> 0 difference\n",
    "- The fourth element in both strings is the same: \"0\" vs. \"0\" -> 0 difference\n",
    "- The fifth element in both strings is different: \"1\" vs. \"1\" -> 0 difference\n",
    "- The sixth element in both strings is different: \"0\" vs. \"1\" -> 1 difference\n",
    "- The seventh element in both strings is the same: \"1\" vs. \"1\" -> 0 difference\n",
    "- The eighth element in both strings is different: \"1\" vs. \"1\" -> 0 difference\n",
    "\n",
    "- Summing up the differences, the Hamming distance between the two strings is 1 + 1 = 2.\n",
    "\n",
    "### Therefore, the Hamming distance (Hamming gap) between the strings \"10001011\" and \"11001111\" is 2.\n",
    "\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fc8b2",
   "metadata": {},
   "source": [
    "Let's first divide them into three features:\n",
    "\n",
    "1. Feature 1: (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "2. Feature 2: (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "3. Feature 3: (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "- The Jaccard index, also known as the Jaccard similarity coefficient, measures the similarity between two sets. It is calculated by dividing the size of the intersection of the sets by the size of their union.\n",
    "\n",
    "- Jaccard_index = |A ∩ B| / |A ∪ B|\n",
    "\n",
    "For Feature 1 and Feature 2:\n",
    "Intersection: (1, 1, 0, 0, 1, 0, 1, 1) ∩ (1, 1, 0, 0, 0, 1, 1, 1) = (1, 1, 0, 0)\n",
    "Union: (1, 1, 0, 0, 1, 0, 1, 1) ∪ (1, 1, 0, 0, 0, 1, 1, 1) = (1, 1, 0, 0, 1, 1, 1, 1)\n",
    "\n",
    "- Jaccard_index = 4 / 8 = 0.5\n",
    "\n",
    "For Feature 1 and Feature 3:\n",
    "Intersection: (1, 1, 0, 0, 1, 0, 1, 1) ∩ (1, 0, 0, 1, 1, 0, 0, 1) = (1, 0, 0, 0, 1, 0, 0, 1)\n",
    "Union: (1, 1, 0, 0, 1, 0, 1, 1) ∪ (1, 0, 0, 1, 1, 0, 0, 1) = (1, 1, 0, 1, 1, 0, 1, 1)\n",
    "\n",
    "- Jaccard_index = 6 / 8 = 0.75\n",
    "\n",
    "For Feature 1 and Feature 3:\n",
    "Matching Elements: (1, 1, 0, 0, 1, 0, 1, 1) ∩ (1, 0, 0, 1, 1, 0, 0, 1) = (1, 0, 0, 0, 1, 0, 0, 1)\n",
    "Total Elements: 8\n",
    "\n",
    "Similarity_Matching_Coefficient = 6 / 8 = 0.75\n",
    "\n",
    "- Jaccard Index (Feature 1 vs. Feature 2): 0.5\n",
    "- Jaccard Index (Feature 1 vs. Feature 3): 0.75\n",
    "- Similarity Matching Coefficient (Feature 1 vs. Feature 2): 0.5\n",
    "- Similarity Matching Coefficient (Feature 1 vs. Feature 3): 0.75\n",
    "\n",
    "### Higher the value, higher the degree of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaf444",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3989d80",
   "metadata": {},
   "source": [
    "### 8. State what is meant by &quot;high-dimensional data set&quot;? \n",
    "\n",
    "### A high-dimensional data set refers to a data set that has a large number of features or dimensions relative to the number of samples or instances. In other words, the data set contains a vast number of variables or attributes, making the data exist in a high-dimensional space.\n",
    "\n",
    "\n",
    "### Could you offer a few real-life examples? \n",
    "\n",
    "### Some examples include \n",
    "\n",
    "- Genomic data with DNA microarrays and next gen sequencing reads\n",
    "- Image processing where one pixel represents a feature\n",
    "\n",
    "### What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "Difficulties in using machine learning techniques on high-dimensional data sets include:\n",
    "\n",
    "1. Curse of Dimensionality: The curse of dimensionality refers to various challenges that arise as the number of dimensions increases. It can lead to sparsity of data, increased computational complexity, and difficulties in finding meaningful patterns or relationships.\n",
    "\n",
    "- Solution: Techniques like Principal Component Analysis (PCA), t-SNE, or feature selection methods can help reduce dimensionality and focus on the most relevant features.\n",
    "\n",
    "2. Overfitting: With high-dimensional data, there is an increased risk of overfitting, where the model learns noise or random patterns instead of true underlying patterns. This can result in poor generalization to new data.\n",
    "\n",
    "- Solution:  Regularization methods, such as L1 (Lasso) or L2 (Ridge) regularization, can help mitigate the risk of overfitting by adding constraints to the model's parameters and encouraging sparsity or small weights.\n",
    "\n",
    "3. Computational Complexity: Many machine learning algorithms become computationally expensive or infeasible to apply directly to high-dimensional data due to the increased number of dimensions. The time and resources required for training and prediction can become prohibitively large.\n",
    "\n",
    "- Solution: Choosing machine learning algorithms specifically designed for high-dimensional data or algorithms that can handle sparsity efficiently can improve performance. Careful evaluation with proper cross-validation techniques such as K-fold CV can help assess the model's performance on high-dimensional data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed79c6",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb4fa3",
   "metadata": {},
   "source": [
    "### 9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Principal Component Analysis.\n",
    "- PCA (Principal Component Analysis) is a widely used dimensionality reduction technique that aims to transform high-dimensional data into a lower-dimensional representation while preserving the most important information.\n",
    "\n",
    "2. Use of vectors\n",
    "- Eigenvectors and Eigenvalues: PCA computes the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, while the eigenvalues indicate the amount of variance explained by each component. Each eigenvector is also represented as a vector.\n",
    "\n",
    "3. Embedded techniques \n",
    "- an embedded technique refers to a method or algorithm that incorporates feature selection or feature extraction within its learning process. These techniques are designed to automatically select or extract relevant features from the input data, improving the model's performance, reducing overfitting, and enhancing interpretability. Some examples are Lasso, Elastic net, Random Forest, Gradient boosting, Deep learning (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75199865",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b1100",
   "metadata": {},
   "source": [
    "### 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection.\n",
    "\n",
    "- Approach: Sequential backward exclusion starts with all features and iteratively removes them, while sequential forward selection starts with an empty set and iteratively adds features.\n",
    "- Number of Features: Sequential backward exclusion reduces the number of features, while sequential forward selection increases the number of features.\n",
    "- Evaluation Criterion: Both methods evaluate the model's performance using a chosen criterion, such as accuracy or error rate.\n",
    "- Flexibility: Sequential forward selection allows the inclusion of more features, which may capture more complex relationships in the data, while sequential backward exclusion focuses on simplifying the model by removing less relevant features.\n",
    "- Computational Efficiency: Sequential forward selection may be computationally more expensive than sequential backward exclusion since it involves evaluating the performance of each individual feature.\n",
    "- Final Feature Subset: The final feature subset obtained by sequential backward exclusion may differ from that obtained by sequential forward selection, as they follow different selection paths.\n",
    "\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper.\n",
    "\n",
    "- Computational Efficiency: Filter methods are generally more computationally efficient as they do not involve training and testing the learning algorithm repeatedly.\n",
    "- Evaluation Criterion: Filter methods use individual feature characteristics as evaluation criteria, while wrapper methods use the performance of the learning algorithm.\n",
    "- Adaptability: Filter methods are more versatile as they can be applied to any learning algorithm, while wrapper methods are specifically designed for a particular learning algorithm.\n",
    "- Overfitting: Wrapper methods are more prone to overfitting as they optimize the feature selection based on the specific learning algorithm and training data, which may result in less generalizable feature subsets.\n",
    "\n",
    "3. SMC vs. Jaccard coefficient.\n",
    "\n",
    "- Calculation Difference: The primary difference between SMC and the Jaccard coefficient lies in their denominators. SMC includes the total number of elements in both sets, while the Jaccard coefficient includes the number of elements in the union of the sets.\n",
    "- Interpretation: Both SMC and the Jaccard coefficient provide a measure of similarity between sets, but their interpretations can differ based on the context and specific data being compared.\n",
    "- Data Type: SMC and the Jaccard coefficient are typically used for binary data or sets, where presence or absence of elements is considered.\n",
    "- Application: SMC and the Jaccard coefficient are widely applied in various fields, including information retrieval, data mining, text analysis, and recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
