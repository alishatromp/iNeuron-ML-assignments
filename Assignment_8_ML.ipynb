{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a76bed6",
   "metadata": {},
   "source": [
    "### 1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfe75d",
   "metadata": {},
   "source": [
    "### In machine learning (ML), a feature refers to an individual measurable property or characteristic of the data that is used as input for a machine learning model. Features are essentially the variables or attributes that represent the input data, and they play a crucial role in helping the model learn patterns and make predictions.\n",
    "\n",
    "### For e.g. Customer Segmentation:\n",
    "- Demographic Features: Age, gender, location, occupation, etc.\n",
    "- Purchase History: Transaction data, such as the frequency and amount of purchases.\n",
    "- Online Behavior: Clickstream data, time spent on different pages, browsing patterns, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fe243",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc787ad3",
   "metadata": {},
   "source": [
    "### 2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9b91b",
   "metadata": {},
   "source": [
    "### Feature construction, also known as feature engineering, is a crucial step in machine learning that involves creating new features or transforming existing ones to improve the performance of a model.\n",
    "\n",
    "### One needs to do feature engineering if \n",
    "- we have insufficient or irrelavent data\n",
    "- Non linearity in the data\n",
    "- dimensionality reduction\n",
    "- Handling categorical variables that needs to be encoded as numerical variables.\n",
    "- Domain knowledge incorporation\n",
    "- Addressing data imbalance\n",
    "- Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfbf86",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b59ed1",
   "metadata": {},
   "source": [
    "### 3. Describe how nominal variables are encoded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b13c31",
   "metadata": {},
   "source": [
    "### When dealing with nominal variables (also known as categorical variables without an inherent order or hierarchy), several encoding techniques can be used to represent them as numerical features. Here are three common methods for encoding nominal variables:\n",
    "\n",
    "1. One-Hot Encoding:\n",
    "\n",
    "In one-hot encoding, each unique category in the nominal variable is transformed into a separate binary feature column.\n",
    "For each instance, only one of these binary columns will have a value of 1, indicating the presence of that category, while the rest will be 0.\n",
    "This encoding preserves the distinctness of each category without imposing any ordinal relationship.\n",
    "One-hot encoding is widely used but can increase the dimensionality of the dataset, especially if the nominal variable has many categories.\n",
    "\n",
    "2. Label Encoding:\n",
    "\n",
    "Label encoding assigns a unique numerical label to each category in the nominal variable.\n",
    "Each category is mapped to an integer value, typically starting from 0 or 1 and incrementing for subsequent categories.\n",
    "Label encoding does not preserve the relative relationships between categories.\n",
    "Some machine learning algorithms might incorrectly interpret these numerical labels as ordered or meaningful, leading to undesired results.\n",
    "Label encoding is suitable when there is an inherent order in the categories (ordinal variables) or when using algorithms that can naturally handle numeric labels.\n",
    "\n",
    "3. Binary Encoding:\n",
    "\n",
    "Binary encoding combines aspects of one-hot encoding and label encoding.\n",
    "Each category is first assigned a unique integer label.\n",
    "The label is then converted to its binary representation.\n",
    "The binary representation is split into separate binary feature columns, each representing a bit.\n",
    "The number of binary feature columns needed is the smallest power of 2 that can represent all the labels.\n",
    "Binary encoding reduces dimensionality compared to one-hot encoding while preserving some information about the order of categories. However, it assumes an ordinal relationship between categories based on the binary representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2235c7e",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a02ce5",
   "metadata": {},
   "source": [
    "### 4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc8b5e",
   "metadata": {},
   "source": [
    "### When converting numerical features to categorical, you can apply a process called binning or discretization. This involves dividing the range of numerical values into distinct intervals or bins and assigning a corresponding category or label to each bin.\n",
    "Examples include:\n",
    "- Equal width binnng\n",
    "- Equal frequency binning\n",
    "- Custom binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bc277",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b0a69",
   "metadata": {},
   "source": [
    "### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012baf9",
   "metadata": {},
   "source": [
    "### The feature selection wrapper approach is a method used to select an optimal subset of features from a larger set of available features. It involves evaluating different subsets of features by training and testing a machine learning model and selecting the subset that leads to the best performance according to a predefined metric. The wrapper approach treats feature selection as a search problem and uses a performance-based criterion to guide the search for the most informative subset of features.\n",
    "\n",
    "### The wrapper approach can be computationally expensive, especially when dealing with a large number of features, as it requires training and evaluating multiple models. However, it offers the advantage of considering the specific predictive power of features in combination with the chosen machine learning algorithm, potentially leading to improved model performance.\n",
    "\n",
    "### It's important to note that the wrapper approach heavily depends on the performance metric and the choice of the evaluation algorithm. Different evaluation algorithms, such as decision trees, support vector machines, or neural networks, can be used within the wrapper approach, depending on the problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696e3b7",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cf482",
   "metadata": {},
   "source": [
    "### 6. When is a feature considered irrelevant? What can be said to quantify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b1bf6",
   "metadata": {},
   "source": [
    "### A feature is considered irrelevant when it does not contribute useful or meaningful information to the task at hand. Irrelevant features can introduce noise, increase computational complexity, and potentially hinder the performance of machine learning models. \n",
    "\n",
    "We can use several ways to quantify it:\n",
    "1. Correlation analysis\n",
    "2. Feature importance/Weight\n",
    "3. Information Gain/Entropy\n",
    "4. Recursive feature elimination\n",
    "5. Expert knowledge/Domain understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d862ce03",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27896a",
   "metadata": {},
   "source": [
    "### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500ec5",
   "metadata": {},
   "source": [
    "### A function is considered redundant when it provides redundant or duplicate information compared to other features already present in the dataset. Redundant features do not contribute new or unique information and can potentially introduce noise, increase computational complexity, and lead to overfitting.\n",
    "\n",
    "We can identify it by:\n",
    "1. Correlation analysis - redundant features show high correlation with each other.\n",
    "2. Mutual information - what two variables share. If mutual info is high, it contains similar information and hence redundancy.\n",
    "3. Variance inflation factor\n",
    "4. Model performance impact\n",
    "5. Domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b4e5c",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c55a8",
   "metadata": {},
   "source": [
    "### 8. What are the various distance measurements used to determine feature similarity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12829880",
   "metadata": {},
   "source": [
    "### There are several distance measurements commonly used to determine feature similarity or dissimilarity between data points.\n",
    "\n",
    "1. Euclidean distance\n",
    "2. Manhattan distance\n",
    "3. Minkoski distance\n",
    "4. Cosine similarity\n",
    "5. Hamming distance\n",
    "6. Jaccard distance\n",
    "7. Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f90bf2",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3421906",
   "metadata": {},
   "source": [
    "### 9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82092cf",
   "metadata": {},
   "source": [
    "- Euclidean Distance: The Euclidean distance is the most commonly used distance metric and measures the straight-line distance between two points in Euclidean space. For a pair of n-dimensional points, it is calculated as the square root of the sum of the squared differences between corresponding feature values.\n",
    "\n",
    "- Manhattan Distance (City Block Distance): The Manhattan distance measures the sum of the absolute differences between corresponding feature values of two points. It is named after the distance a taxi would travel in a city block to go from one point to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79db10",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b805",
   "metadata": {},
   "source": [
    "### 10. Distinguish between feature transformation and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cfce5",
   "metadata": {},
   "source": [
    "### Feature Transformation:\n",
    "\n",
    "- Feature transformation involves applying mathematical or statistical operations to the existing features to create new representations of the data.\n",
    "- It aims to capture underlying patterns, reduce noise, or make the data more suitable for the chosen machine learning algorithm.\n",
    "- Feature transformation techniques include scaling, normalization, logarithmic transformation, polynomial transformation, principal component analysis (PCA), and other dimensionality reduction methods.\n",
    "- Feature transformation modifies the original features but retains all or most of them in the transformed dataset.\n",
    "- The transformed features carry the same semantic meaning as the original features but might have different distributions or representations.\n",
    "\n",
    "### Feature Selection:\n",
    "\n",
    "- Feature selection involves selecting a subset of relevant features from the original set of available features.\n",
    "- It aims to identify the most informative and discriminative features for the prediction task while discarding irrelevant or redundant features.\n",
    "- Feature selection techniques evaluate the relevance or importance of each feature and rank them based on certain criteria.\n",
    "- Examples of feature selection methods include filter methods (e.g., correlation, mutual information), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., L1 regularization).\n",
    "- Feature selection reduces the dimensionality of the dataset by eliminating unnecessary features, which can enhance model performance, reduce computational complexity, and improve interpretability.\n",
    "- The selected features might have a different semantic interpretation than the original features, as some of the original features might be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91281e",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f417fa2",
   "metadata": {},
   "source": [
    "### 11. Make brief notes on any two of the following:\n",
    "\n",
    "1. SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "- When it comes to collecting features using a hybrid method, it generally means combining multiple approaches or techniques for feature collection\n",
    "- Domain Knowledge is a hybrid approach - Start by leveraging domain knowledge and subject matter expertise to identify relevant features. Experts in the field can provide valuable insights into the key variables or attributes that are likely to be important for the problem at hand. Do a literature review. Use automated feature extraction techniques and statistical analysis. Machine learning feature importance and iterative refinement. \n",
    "\n",
    "3. The width of the silhouette\n",
    "- The silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
