{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d6cf32",
   "metadata": {},
   "source": [
    "### 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa077ea",
   "metadata": {},
   "source": [
    "Supervised Learning:\n",
    "Supervised learning involves training a model using labeled data. Labeled data consists of input samples and corresponding output labels, which serve as the ground truth for the model to learn from. The goal is for the model to generalize and make accurate predictions on unseen data. Supervised learning algorithms can be categorized into two main types:\n",
    "\n",
    "- Classification: In classification tasks, the goal is to predict the class or category of an input sample. For example, given a dataset of emails labeled as \"spam\" or \"not spam,\" a supervised learning algorithm can be trained to classify new, unseen emails as either spam or not spam.\n",
    "- Regression: In regression tasks, the goal is to predict a continuous numerical value or a function approximation. For instance, a supervised learning algorithm can be used to predict housing prices based on factors such as square footage, number of bedrooms, and location.\n",
    "\n",
    "\n",
    "Unsupervised learning, on the other hand, involves training a model on unlabeled data. In this case, the algorithm aims to discover patterns, structures, or relationships within the data without any predefined labels. The model learns to group or cluster similar instances together based on their inherent characteristics. Unsupervised learning algorithms are particularly useful when the underlying structure or patterns in the data are unknown.\n",
    "We use these to demonstrate unsupervised machine learning.\n",
    "\n",
    "- Clustering is a common unsupervised learning technique where the goal is to identify groups or clusters of similar data points. For example, an unsupervised learning algorithm can be applied to customer segmentation, grouping customers into distinct clusters based on their purchasing behavior.\n",
    "-  Dimensionality reduction techniques aim to reduce the number of features or variables in the data while retaining important information. Principal Component Analysis (PCA) is a popular unsupervised learning algorithm used for dimensionality reduction.\n",
    "- Anomaly detection, where the goal is to identify rare or abnormal instances in a dataset. For instance, in fraud detection, unsupervised learning algorithms can be used to identify unusual patterns that deviate from normal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cb34b",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729486d6",
   "metadata": {},
   "source": [
    "### 2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a3277",
   "metadata": {},
   "source": [
    "1. Unsupervised learning algorithms can be used to cluster customers based on their purchasing behavior, demographic information, or browsing patterns. \n",
    "2. It can be employed to cluster similar images or documents together. This can be useful in organizing large image or document collections, information retrieval, or content recommendation systems.\n",
    "3. To detect anomalies or outliers in datasets. This is valuable in fraud detection, network intrusion detection, identifying faulty equipment, or identifying unusual patterns in healthcare data for early disease detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bedb3c",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f6de6",
   "metadata": {},
   "source": [
    "### 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026a507",
   "metadata": {},
   "source": [
    "1. Hierarchical clustering: Hierarchical clustering creates a hierarchy of clusters by iteratively merging or splitting clusters based on their similarity. It does not require the number of clusters to be predefined. The result is a tree-like structure called a dendrogram, which shows the relationships between clusters at different levels of similarity.\n",
    "\n",
    "2. K-means clustering partitions the data into a predefined number of clusters, where 'k' represents the number of clusters. Each instance is assigned to the cluster with the nearest mean (centroid) based on the distance metric, usually Euclidean distance.\n",
    "\n",
    "3. Density-based clustering aims to find regions of high-density separated by regions of low-density in the data space. It does not require specifying the number of clusters in advance and can discover clusters of arbitrary shape and size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f193684",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40e1b9",
   "metadata": {},
   "source": [
    "### 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a3625",
   "metadata": {},
   "source": [
    "The k-means algorithm does not explicitly determine the consistency of clustering. Instead, it aims to minimize the within-cluster sum of squares, also known as the inertia or distortion, as a measure of clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1fcb1",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d168d1",
   "metadata": {},
   "source": [
    "### 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cfd429",
   "metadata": {},
   "source": [
    "Both are ways used for clustering, but they differ in the way they determine the cluster centroids. \n",
    "for e.g. we have a dataset such as \n",
    "A (1,2), B (2,4), C(3,6), D(8,7), E(9,6). Performing k-means with k=2, we aim to find two clusters. Performing the necessary steps we converge to:\n",
    "\n",
    "- Cluster 1: A,B,C (Centroid (2,4))\n",
    "- Cluster 2: D,E (Centroid (8.5, 6.5))\n",
    "\n",
    "In k-means, the centroids are calculated as the mean of the instances within each cluster. The cluster representatives (centroids) do not necessarily correspond to actual instances in the dataset.\n",
    "\n",
    "Now for k-medoids:\n",
    "\n",
    "k-medoids algorithm converges, and we get the following assignments and medoids:\n",
    "\n",
    "Cluster 1: A, B, C (Medoid: A (1, 2))\n",
    "Cluster 2: D, E (Medoid: D (8, 7))\n",
    "\n",
    "In k-medoids, the medoids are actual instances selected from the dataset, serving as representatives of their respective clusters. Unlike k-means, the medoids need to be one of the instances in the dataset. This distinction makes k-medoids more robust to outliers and suitable for cases where the centroids cannot be calculated as means (e.g., with non-numerical or categorical data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f60da8",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635c908",
   "metadata": {},
   "source": [
    "### 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c026e7",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-like structure used to visualize hierarchical clustering results. It represents the relationships and similarity levels between different clusters and data points. Dendrograms are commonly employed in agglomerative hierarchical clustering algorithms.\n",
    "\n",
    "1. Start with individual data points that need clustering.\n",
    "2. Compute the distance or dissimilarity measure between each pair of data points. \n",
    "3. Identify the pair of clusters with the shortest distance or highest similarity and merge them into a single cluster. This step is repeated iteratively, gradually forming larger clusters.\n",
    "4. Record the distance or similarity level between the merged clusters.\n",
    "5. Each cluster is represented as a branch on the dendrogram, and the data points are represented as the endpoints of the branches.\n",
    "6. Start with individual data points as the leaves of the dendrogram. For each iteration of merging clusters, join the corresponding branches on the dendrogram at the specified distance or similarity level. \n",
    "7. Analyze the resulting dendrogram to understand the hierarchical structure of the data. The height at which branches merge represents the distance or dissimilarity level at which clusters are combined. Closer clusters on the dendrogram share more similarity or have a smaller distance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90b9c1",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640651c",
   "metadata": {},
   "source": [
    "### 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104b936",
   "metadata": {},
   "source": [
    "SSE stands for \"Sum of Squared Errors\" or \"Sum of Squared Distances.\" It is a measure used to evaluate the quality of clustering in the k-means algorithm. SSE quantifies the overall compactness or tightness of the clusters by summing the squared distances between each instance and its assigned centroid within each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04ed0f",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90e24a",
   "metadata": {},
   "source": [
    "### 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37bcfa",
   "metadata": {},
   "source": [
    "1. Decide how many clusters you want to create in the dataset.\n",
    "2. Randomly select k data points from the dataset as the initial cluster centroids.\n",
    "3. Calculate the Euclidean distance for each data point.\n",
    "4. Assign each data point to the cluster with the nearest centroid based on the calculated distances.\n",
    "5. Calculate the new centroid for each cluster by taking the mean of the data points assigned to that cluster.\n",
    "6. Repeat steps until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aad460",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1339a0",
   "metadata": {},
   "source": [
    "### 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafb562",
   "metadata": {},
   "source": [
    "Single link measures the distance between two clusters as the shortest distance between any two points, one from each cluster.\n",
    "\n",
    "Complete link measures the distance between two clusters as the maximum distance between any two points, one from each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c3894",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84bc61",
   "metadata": {},
   "source": [
    "### 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69445f6c",
   "metadata": {},
   "source": [
    "The Apriori concept is a data mining technique that aids in reducing measurement overhead in business basket analysis. It accomplishes this by employing a priori knowledge or assumptions to focus on frequent itemsets, which are combinations of items that occur together frequently in a dataset. By targeting these frequent itemsets, the Apriori concept reduces the number of measurements required, making the analysis more efficient.\n",
    "\n",
    "For example, if there are 100 different items in the supermarket, measuring the support for all possible pairs, triplets, and larger combinations of items would require a large number of measurements.\n",
    "It starts by measuring the support of individual items like 'bread', 'milk', 'eggs'. It then selects only those itemsets that meet a min support threshold (e.g. 5% of transactions). \n",
    "\n",
    "Next it itemsets of size two such as {bread,milk}, {bread,eggs} and {milk,eggs}. The algorithm measures the support of these candidate itemsets and again selects the frequent ones.\n",
    "\n",
    "By utilizing the Apriori concept, the analysis focuses on measuring the support only for the potentially frequent itemsets, avoiding unnecessary measurements for infrequent or unlikely combinations. This reduction in measurement overhead makes the basket analysis more efficient, especially when dealing with large datasets or a large number of potential item combinations.\n",
    "\n",
    "Overall, the Apriori concept helps optimize basket analysis by targeting frequent itemsets and reducing measurement overhead, allowing businesses to identify significant associations and patterns in customer purchasing behavior more efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
