{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5d8ed7",
   "metadata": {},
   "source": [
    "### 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95426199",
   "metadata": {},
   "source": [
    "Used for both classification and regression tasks, the primary idea behind SVM is to find the optimal hyperplane that best separates different classes of data in a high-dimensional space. SVM creates a decision boundary that best seperates data points from different classes with the widest possible margin. \n",
    "\n",
    "This results in a robust and efficient classifier, particularly useful in cases with high-dimensional data or non-linearly separable classes. SVM has been widely used in various fields, including image classification, text categorization, and bioinformatics, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbc8e8",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77746a",
   "metadata": {},
   "source": [
    "### 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e59dd",
   "metadata": {},
   "source": [
    "The concept of a support vector is a fundamental idea in SVM. In SVM, a support vector refers to the data points from the training dataset that are closest to the decision boundary (hyperplane). These are the data points that lie on or within the margin, which is the region between the hyperplane and the closest data points of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0627e",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ed949",
   "metadata": {},
   "source": [
    "### 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee280c",
   "metadata": {},
   "source": [
    "SVM aims to find the hyperplane that best separates the data points of different classes. The position and orientation of the decision boundary are influenced by the scale of the input features.\n",
    "If the features have different scales, some dimensions may dominate the decision-making process, while others may be largely ignored. Scaling ensures that all features contribute more equally to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f6f92",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9146660",
   "metadata": {},
   "source": [
    "### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb37bd",
   "metadata": {},
   "source": [
    "Yes, it can.\n",
    "\n",
    "By default, SVMs are not direct probability estimators like logistic regression, and they do not provide a straightforward probability score representing the confidence of a particular classification. Instead, SVMs are primarily designed for binary classification, and their decision is based on the sign of the output value from the decision function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a14c8d",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7f0ee",
   "metadata": {},
   "source": [
    "### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37983d09",
   "metadata": {},
   "source": [
    "If you have a dataset with millions of instances and hundreds of features, and the number of instances is significantly larger than the number of features, you might want to consider using the primal form of the SVM. On the other hand, if the number of features is much larger than the number of instances, the dual form could be a better choice, especially if you plan to leverage kernel methods for non-linear classification. As always, it's a good practice to experiment with both approaches and measure their performance and computational efficiency on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a36aa3",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2bde83",
   "metadata": {},
   "source": [
    "### 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f193360",
   "metadata": {},
   "source": [
    "When an SVM classifier with an RBF kernel appears to underfit the training data, you need to adjust the hyperparameters gamma and C to improve its performance.\n",
    "\n",
    "The gamma parameter determines the influence of a single training example. It controls the shape of the decision boundary.\n",
    "- Higher gamma values make the decision boundary more complex and can lead to overfitting, where the model memorizes the training data but performs poorly on unseen data.\n",
    "- Lower gamma values make the decision boundary smoother and can lead to underfitting, where the model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "Therefore to address underfitting - we need to increase gamma value.\n",
    "\n",
    "The C parameter is the regularization parameter in SVM, which controls the trade-off between maximizing the margin (finding the widest possible margin) and minimizing the classification error on the training data.\n",
    "\n",
    "- A larger C value allows the model to have fewer margin violations (misclassifications) in the training data, which can lead to overfitting.\n",
    "- A smaller C value allows the model to have a larger margin and allows more margin violations, which can lead to underfitting.\n",
    "\n",
    "To address underfitting you need to increase the C value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5517ab9",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc604cf",
   "metadata": {},
   "source": [
    "### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c2c23",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem using an off-the-shelf Quadratic Programming (QP) solver, you need to formulate the QP problem in the standard form and set the corresponding parameters (H, f, A, and b).\n",
    "\n",
    "Using the formulation \n",
    "(1/2) * α^T * H * α - f^T * α\n",
    "we set the parameters accordingly and then use an off-the-shelf QP solver to find the optimal values of α. From there, you can recover the optimal values of w and b to obtain the linear SVM classifier.\n",
    "\n",
    "For e.g. - An off-the-shelf QP solver called \"Quadprod\" is a Python library. \n",
    "In the Quadprog library, you can set the QP parameters using the function quadprog.solve_qp, which allows you to solve a quadratic programming problem of the form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7a358",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dad570",
   "metadata": {},
   "source": [
    "### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset and train LinearSVC\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "linear_svc_model = LinearSVC()\n",
    "linear_svc_model.fit(X_train, y_train)\n",
    "\n",
    "#Train SVC. Since data is linearly separable, set kernel to linear.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(kernel='linear')\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#The SGD classifier is a stochastic classifier. Set loss parameter to hinge to make it behave like SVM.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier(loss='hinge')\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "#Model comparison\n",
    "linear_svc_accuracy = linear_svc_model.score(X_test, y_test)\n",
    "svc_accuracy = svc_model.score(X_test, y_test)\n",
    "sgd_accuracy = sgd_model.score(X_test, y_test)\n",
    "\n",
    "#To evaluate the similarity between the models, you can compare their\n",
    "#accuracy on a validation set or test set. \n",
    "#Use the score method to get the accuracy.\n",
    "\n",
    "print(\"LinearSVC Accuracy:\", linear_svc_accuracy)\n",
    "print(\"SVC Accuracy:\", svc_accuracy)\n",
    "print(\"SGDClassifier Accuracy:\", sgd_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d084b7",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731a4c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "### Running this code below will take forever as the MNIST data is huge to dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Scale the features to a range between 0 and 1\n",
    "X = X / 255.0\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the SVM classifier with a linear kernel\n",
    "svm_classifier = SVC(kernel='linear', decision_function_shape='ovr', random_state=42)\n",
    "\n",
    "# Define a range of C values to tune the hyperparameter\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C = None\n",
    "\n",
    "# Hyperparameter tuning using validation set\n",
    "for C in C_values:\n",
    "    svm_classifier.C = C\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_val_pred = svm_classifier.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_C = C\n",
    "\n",
    "# Train the SVM classifier with the best hyperparameter on the full training set\n",
    "svm_classifier.C = best_C\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = svm_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Best C value: {best_C}\")\n",
    "print(f\"Validation Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5b07e",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4e158",
   "metadata": {},
   "source": [
    "### 10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9ac5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5792049946323379\n",
      "R-squared Score: 0.5579967748619474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset and perform any preprocessing such as scaling or handling missing values.\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create an SVM regressor (you can adjust kernel and other hyperparameters)\n",
    "svm_regressor = SVR(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the SVM regressor on the scaled training data\n",
    "svm_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate mean squared error and R-squared score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
