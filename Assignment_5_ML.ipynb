{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ebd52e",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32fe22",
   "metadata": {},
   "source": [
    "### The machine learning (ML) workflow refers to the general sequence of steps involved in developing and deploying a machine learning model. The workflow typically includes several key stages that encompass tasks such as data preparation, model training, evaluation, and deployment. \n",
    "\n",
    "### Preprocessing in machine learning implies to the steps and techniques applied to raw data before it is used for training a model. It involves transforming the data into a format that is suitable for the machine learning algorithms and improving the quality and relevance of the data. Preprocessing plays a critical role in ensuring accurate and reliable model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608d331",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86a8ed",
   "metadata": {},
   "source": [
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a3c09",
   "metadata": {},
   "source": [
    "### Qualitative and quantitative data are two distinct types of data used in research and analysis. They differ in their nature, characteristics, and the methods used to collect and analyze them. \n",
    "\n",
    "- Qualitative data consists of non-numerical information that describes qualities, characteristics, opinions, beliefs, behaviors, or experiences. It is subjective and focuses on understanding the meaning, context, and depth of phenomena. Qualitative data is typically recorded as text, transcripts, audio recordings, images, or videos. It is unstructured and requires interpretation and coding to extract themes or patterns. Qualitative data analysis involves interpreting and making sense of the data through techniques like thematic analysis, content analysis, or grounded theory. It aims to identify themes, relationships, or emerging patterns to generate rich, descriptive insights.\n",
    "\n",
    "- Quantitative data consists of numerical information that can be measured or counted objectively. It focuses on collecting empirical evidence and establishing relationships between variables through statistical analysis.It is represented as numbers, counts, or statistical measures. It follows a structured format, often organized in tables, spreadsheets, or databases, making it amenable to mathematical and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25c6a8",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4c093",
   "metadata": {},
   "source": [
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f06a2",
   "metadata": {},
   "source": [
    "1. Numeric Data Type:\n",
    "\n",
    "Attribute: Age\n",
    "Sample Records:\n",
    "Record 1: Age = 25\n",
    "Record 2: Age = 42\n",
    "Record 3: Age = 37\n",
    "\n",
    "2. Categorical Data Type:\n",
    "\n",
    "Attribute: Gender\n",
    "Sample Records:\n",
    "Record 1: Gender = Male\n",
    "Record 2: Gender = Female\n",
    "Record 3: Gender = Other\n",
    "\n",
    "3. Text Data Type:\n",
    "\n",
    "Attribute: Occupation\n",
    "Sample Records:\n",
    "Record 1: Occupation = Engineer\n",
    "Record 2: Occupation = Teacher\n",
    "Record 3: Occupation = Doctor\n",
    "\n",
    "4. Boolean Data Type:\n",
    "\n",
    "Attribute: Subscription\n",
    "Sample Records:\n",
    "Record 1: Subscription = True\n",
    "Record 2: Subscription = False\n",
    "Record 3: Subscription = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e65a2b",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59005c55",
   "metadata": {},
   "source": [
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f249c1",
   "metadata": {},
   "source": [
    "### There are several causes of machine learning data issues that can have significant ramifications on the performance and reliability of machine learning models. Here are some common causes and their associated ramifications:\n",
    "\n",
    "1. Insufficient or Biased Data:\n",
    "\n",
    "Cause: When the training data is inadequate in terms of size or representation, or when it contains inherent biases.\n",
    "Ramifications: Models trained on insufficient or biased data may suffer from poor generalization, limited accuracy, and biased predictions. They may fail to capture the true complexity of the underlying problem or make unfair decisions.\n",
    "\n",
    "2. Noisy or Inaccurate Data:\n",
    "\n",
    "Cause: Data that contains errors, outliers, or inconsistencies, either due to measurement errors or human input.\n",
    "Ramifications: Noisy data can mislead the model, leading to incorrect predictions and reduced performance. Models may overfit to outliers, leading to poor generalization. It can also affect the interpretability and trustworthiness of the model's outputs.\n",
    "\n",
    "3. Missing Data:\n",
    "\n",
    "Cause: When certain attributes or values are missing from the dataset, either due to data collection issues or data entry errors.\n",
    "Ramifications: Missing data can introduce biases, reduce the representativeness of the dataset, and hinder model training. Models may struggle to handle missing values during prediction, leading to inaccurate results.\n",
    "\n",
    "4. Imbalanced Data:\n",
    "\n",
    "Cause: When the distribution of classes or target variables in the dataset is highly skewed, with one class being significantly more prevalent than others.\n",
    "Ramifications: Imbalanced data can cause models to be biased towards the majority class, resulting in poor performance on minority classes. Models may struggle to learn from underrepresented classes, leading to low recall and compromised accuracy for those classes.\n",
    "\n",
    "5. Irrelevant or Redundant Features:\n",
    "\n",
    "Cause: Including features that have little or no predictive power, or features that are highly correlated with other features.\n",
    "Ramifications: Irrelevant or redundant features can introduce noise, increase the complexity of the model, and potentially lead to overfitting. They can hinder the model's ability to identify meaningful patterns and make accurate predictions.\n",
    "\n",
    "6. Data Leakage:\n",
    "\n",
    "Cause: When information from the test or future data inadvertently leaks into the training data, leading to unrealistic performance during model evaluation.\n",
    "Ramifications: Data leakage can give an inflated sense of model performance, making it difficult to assess the model's true generalization ability. The model may fail to perform well on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c403e5",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a168e1",
   "metadata": {},
   "source": [
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c23e1d",
   "metadata": {},
   "source": [
    "### When exploring categorical data, there are several approaches you can take to gain insights and understand the distribution and patterns within the data. Here are some common approaches with appropriate examples:\n",
    "\n",
    "1. Frequency Distribution:\n",
    "\n",
    "Calculate the frequency or count of each category in the dataset.\n",
    "Example: Suppose you have a dataset of customer reviews for a product, and one of the categorical variables is \"Rating\" with categories: Poor, Average, Good, Excellent. You can compute the frequency of each rating category to understand the distribution, such as Poor: 50 reviews, Average: 100 reviews, Good: 200 reviews, Excellent: 150 reviews.\n",
    "\n",
    "2. Bar Plot:\n",
    "\n",
    "Visualize the frequency distribution using a bar plot.\n",
    "Example: Using the above example, you can create a bar plot where the x-axis represents the rating categories (Poor, Average, Good, Excellent), and the y-axis represents the count of reviews. This plot visually displays the distribution of ratings and allows for easy comparison between categories.\n",
    "\n",
    "3. Cross-Tabulation:\n",
    "\n",
    "Create a cross-tabulation or contingency table to explore the relationship between two categorical variables.\n",
    "Example: Consider a dataset of students' performance with two categorical variables: \"Gender\" (Male, Female) and \"Pass/Fail\" (Yes, No). You can create a cross-tabulation table to examine the count of students who passed or failed based on gender, providing insights into potential gender-based differences in performance.\n",
    "\n",
    "4. Stacked Bar Plot:\n",
    "\n",
    "Visualize the cross-tabulation results using a stacked bar plot to visualize the relationship between two categorical variables.\n",
    "Example: Using the previous example, you can create a stacked bar plot where the x-axis represents gender, the y-axis represents the count of students, and the bars are stacked to represent the count of students who passed and failed, allowing for a comparison of pass/fail proportions between genders.\n",
    "\n",
    "5. Pie Chart:\n",
    "\n",
    "Use a pie chart to represent the proportion or percentage distribution of categories within a categorical variable.\n",
    "Example: Suppose you have a dataset of customer preferences for different types of beverages: Coffee, Tea, Soda, Juice. You can create a pie chart where each category is represented by a slice of the pie, and the size of each slice corresponds to the proportion or percentage of customers who prefer that beverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da696b0",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c051970",
   "metadata": {},
   "source": [
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db18567",
   "metadata": {},
   "source": [
    "### When certain variables have missing values, the learning activity can be affected in several ways:\n",
    "\n",
    "1. Reduced Sample Size: Missing values in variables lead to a reduction in the effective sample size available for analysis. This can impact the statistical power of the analysis and may limit the generalizability of the results.\n",
    "\n",
    "2. Biased Estimates: If the missingness is related to the variable itself or other variables in the dataset, it can introduce bias in the estimates and distort the relationships between variables. This can lead to inaccurate conclusions and unreliable predictions.\n",
    "\n",
    "3. Incomplete Analysis: Missing values can lead to incomplete analysis or exclusion of certain variables from the analysis altogether. This can result in an incomplete understanding of the underlying phenomena or overlook important factors that could contribute to the analysis.\n",
    "\n",
    "To deal with missing values one can \n",
    "- Delete rows or columns of missing values.\n",
    "- Apply mean/median/mode to estimate missing values.\n",
    "- Apply ML algorithms to estimate missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c713653",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321ac35",
   "metadata": {},
   "source": [
    "### 7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9abb",
   "metadata": {},
   "source": [
    "### Dealing with missing data values is an important step in data preprocessing to ensure the accuracy and reliability of machine learning models. Here are several commonly used methods for handling missing data:\n",
    "\n",
    "1. Deletion:\n",
    "\n",
    "Listwise Deletion: Removing entire rows that contain missing values. This method can lead to a loss of information if the missing values are not randomly distributed.\n",
    "Pairwise Deletion: Conducting analyses using available data for each specific pair of variables. This method retains more information but can introduce bias if the missingness is related to the variables being analyzed.\n",
    "\n",
    "2. Imputation:\n",
    "\n",
    "Mean/Median/Mode Imputation: Replacing missing values with the mean (for numeric data), median (for skewed data), or mode (for categorical data) of the respective variable. This method assumes that missing values are missing completely at random (MCAR) or missing at random (MAR). However, it can underestimate variance and distort relationships between variables.\n",
    "Regression Imputation: Predicting missing values using regression models based on other variables. This method can preserve relationships between variables but introduces additional uncertainty if the regression model is not accurate.\n",
    "Hot Deck Imputation: Replacing missing values with observed values from similar cases, using methods such as nearest neighbor matching or random selection. This method attempts to preserve the original distribution but may not account for relationships between variables.\n",
    "Multiple Imputation: Generating multiple imputed datasets based on the observed data and using them for analysis. This method accounts for the uncertainty associated with missing data and provides more accurate estimates. It involves creating imputations using regression models or other sophisticated techniques and combining the results.\n",
    "\n",
    "3. Indicator/Dummy Variable:\n",
    "\n",
    "Creating an additional binary indicator variable to indicate whether the data is missing or not. This approach preserves the original dataset while allowing the model to consider the missingness as a separate category. It can be useful when the missingness itself contains valuable information.\n",
    "\n",
    "4. Advanced Techniques:\n",
    "\n",
    "Expectation-Maximization (EM): An iterative algorithm that estimates missing values by imputing them based on the expected distribution. It is particularly effective for dealing with missing data that follows a specific pattern.\n",
    "Machine Learning-Based Imputation: Utilizing machine learning models, such as k-nearest neighbors (KNN) or random forests, to predict missing values based on other variables. This approach can capture complex relationships but requires substantial computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd41c06",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17d4fa",
   "metadata": {},
   "source": [
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66952f8b",
   "metadata": {},
   "source": [
    "### Various data pre-processing techniques are employed to prepare raw data for machine learning algorithms. Some commonly used techniques include:\n",
    "\n",
    "Data Cleaning: Handling missing data, dealing with outliers, correcting errors, and addressing inconsistencies in the dataset.\n",
    "\n",
    "Data Transformation: Applying transformations such as normalization (scaling data to a standard range), logarithmic transformation (handling skewed data), or encoding categorical variables (converting categorical data into numerical representations).\n",
    "\n",
    "Feature Selection: Identifying and selecting the most relevant subset of features from the dataset that have the most significant impact on the target variable.\n",
    "\n",
    "Dimensionality Reduction: Reducing the number of input features or variables while preserving the essential information. It helps in reducing computational complexity, mitigating the curse of dimensionality, and improving the performance and interpretability of machine learning models.\n",
    "\n",
    "Dimensionality Reduction: Dimensionality reduction techniques aim to reduce the number of input features while preserving the relevant information. It helps in overcoming issues related to high-dimensional data, such as computational complexity, overfitting, and difficulty in visualizing and interpreting data. There are two main approaches to dimensionality reduction:\n",
    "\n",
    "- Feature Selection: This approach involves selecting a subset of the original features based on their relevance to the target variable or their ability to capture the underlying patterns in the data. Common methods for feature selection include statistical tests, correlation analysis, information gain, and recursive feature elimination.\n",
    "\n",
    "- Feature Extraction: Feature extraction transforms the original set of features into a new set of lower-dimensional features. It aims to find a set of derived features that capture most of the information in the original data. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are popular techniques for feature extraction.\n",
    "\n",
    "Both feature selection and feature extraction techniques help in reducing dimensionality and improving the performance and interpretability of machine learning models by focusing on the most relevant and informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690ec9c",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ccd70",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker\n",
    "surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35627bbc",
   "metadata": {},
   "source": [
    "### The IQR, or Interquartile Range, is a statistical measure used to describe the spread or dispersion of a dataset. It is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) of the dataset. The IQR represents the range of the middle 50% of the data, specifically the range between the 25th and 75th percentiles.\n",
    "\n",
    "### The IQR is commonly used in conjunction with the box-and-whisker plot to visualize and assess the distribution of a dataset. The box in the plot represents the IQR, with the median (50th percentile) indicated by a line within the box. The whiskers extend to the minimum and maximum values within a certain range, typically 1.5 times the IQR.\n",
    "\n",
    "### The lower whisker will be longer than the upper whisker when the data is left skewed.\n",
    "\n",
    "### The IQR is particularly useful for identifying outliers in a dataset. Outliers are typically defined as data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. However, the choice of the specific criteria for defining outliers may vary depending on the context and the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd052298",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d1bf9",
   "metadata": {},
   "source": [
    "### 10. Make brief notes on any two of the following:\n",
    "\n",
    "1. Data collected at regular intervals\n",
    "\n",
    "2. The gap between the quartiles\n",
    "\n",
    "3. Use a cross-tab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ec274",
   "metadata": {},
   "source": [
    "1. Data collected at regular intervals\n",
    "\n",
    "Interval data is a type of quantitative data that represents measurements or values where the intervals between the values are meaningful and consistent. It is characterized by having equal intervals between adjacent values, and the data points are ordered along a continuous numerical scale.\n",
    "\n",
    "In interval data, the differences between values have a fixed unit of measurement, but there is no true zero point or absolute origin. This means that while the numerical values can be added, subtracted, multiplied, or divided, they cannot be interpreted in terms of ratios or proportions.\n",
    "\n",
    "A common example of interval data is temperature measured on the Celsius or Fahrenheit scale. The difference between 20°C and 30°C is the same as the difference between 30°C and 40°C, indicating a consistent interval. However, it is not meaningful to say that 40°C is twice as hot as 20°C, as zero does not represent the absence of temperature.\n",
    "\n",
    "2. The gap between the quartiles\n",
    "\n",
    "The gap between quartiles refers to the difference between the values of the first quartile (Q1) and the third quartile (Q3) in a dataset. The quartiles divide the dataset into four equal parts, with Q1 representing the 25th percentile and Q3 representing the 75th percentile.\n",
    "\n",
    "The gap between quartiles, often referred to as the interquartile range (IQR), is calculated as:\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "The IQR is a measure of the spread or dispersion of the middle 50% of the data. It provides valuable information about the variability within the dataset without being influenced by extreme values or outliers.\n",
    "\n",
    "The IQR is commonly used in statistical analysis and data visualization, particularly in box-and-whisker plots. In a box plot, the box represents the IQR, with the median (50th percentile) indicated by a line within the box. The whiskers extend to the minimum and maximum values within a certain range, typically 1.5 times the IQR.\n",
    "\n",
    "The gap between quartiles, or the IQR, is a robust measure of spread that is less affected by outliers compared to other measures such as the range or standard deviation. It provides insights into the central tendency and variability of the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66cd8f",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefb65b",
   "metadata": {},
   "source": [
    "### 11. Make a comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values\n",
    "\n",
    "Nominal data has no inherent order, while ordinal data has a meaningful order or ranking.\n",
    "Nominal data uses discrete and unordered categories, while ordinal data uses discrete categories with a meaningful order.\n",
    "Nominal data focuses on frequency counts and modes, while ordinal data considers ranks, medians, and non-parametric tests.\n",
    "Ordinal data provides more information than nominal data, as it includes both the categories and their relative order.\n",
    "\n",
    "2. Histogram and box plot\n",
    "\n",
    "Histogram presents the data distribution using bars, while a box plot summarizes the data using a box and potentially whiskers.\n",
    "Histogram provides a more detailed view of the data distribution, while a box plot focuses on summary statistics like quartiles and outliers.\n",
    "Histogram may not explicitly show outliers, whereas a box plot often represents outliers as individual points or whiskers.\n",
    "Histogram is useful for comparing the shape and patterns of distributions between different groups, while a box plot facilitates easy comparison of the central tendency and spread across groups.\n",
    "\n",
    "3. The average and median\n",
    "\n",
    "The mean, also known as the arithmetic average, is calculated by summing all the values in a dataset and dividing by the total number of observations.\n",
    "The median is the middle value in an ordered dataset. It divides the dataset into two equal halves, where 50% of the observations are above the median and 50% are below it.\n",
    "\n",
    "The mean is sensitive to extreme values, while the median is not significantly influenced by outliers.\n",
    "The mean represents the balance point of the data, while the median represents the central value.\n",
    "The mean is more influenced by the shape of the distribution, while the median is less affected and provides a robust measure.\n",
    "The mean considers all values in the dataset, while the median only depends on the position of the middle value(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
