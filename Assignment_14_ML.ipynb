{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d79e22f",
   "metadata": {},
   "source": [
    "### 1. What is the concept of supervised learning? What is the significance of the name?\n",
    "\n",
    "Supervised learning is a machine learning paradigm where an algorithm learns from labeled training data to make predictions or take actions. In supervised learning, a dataset is provided that consists of input data (features) and corresponding output labels or target values. The goal is to train a model that can generalize from the labeled examples and accurately predict the correct labels for new, unseen data.\n",
    "\n",
    "The name \"supervised learning\" reflects the nature of the learning process. It involves a \"supervisor\" or \"teacher\" who provides the algorithm with the correct answers (labels) during training. The algorithm learns to map the input data to the correct output labels by minimizing the error or difference between its predictions and the provided labels. Through this process, the algorithm aims to generalize the patterns observed in the labeled examples to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034270a3",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65751832",
   "metadata": {},
   "source": [
    "### 2. In the hospital sector, offer an example of supervised learning.\n",
    "\n",
    "In the hospital sector, one example of supervised learning is predicting patient readmission. By utilizing historical patient data, including demographics, medical history, and treatment information, a supervised learning algorithm can be trained to predict whether a patient is likely to be readmitted within a specific time frame after being discharged from the hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0712bdc",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679be4c",
   "metadata": {},
   "source": [
    "### 3. Give three supervised learning examples.\n",
    "\n",
    "1. Spam Email Classification:\n",
    "Supervised learning can be used to classify emails as either spam or non-spam (ham). By training a model on a dataset of labeled emails, where each email is labeled as spam or non-spam, the algorithm can learn patterns and features that distinguish between the two classes. Once trained, the model can predict whether new, unseen emails are likely to be spam or not.\n",
    "\n",
    "2. Credit Risk Assessment:\n",
    "Supervised learning can assist in assessing credit risk for loan applicants. By using historical loan data that includes information about applicants' financial history, employment status, credit scores, and other relevant features, a supervised learning model can be trained to predict whether an applicant is likely to default on their loan payments. This can help financial institutions make informed decisions about approving or denying loan applications.\n",
    "\n",
    "3. Medical Diagnosis:\n",
    "Supervised learning can be applied to medical diagnosis tasks, such as identifying diseases or conditions based on patient symptoms and medical test results. By training a model on a dataset of labeled cases, where each case includes patient features (symptoms, medical history, test results) and the corresponding diagnosed condition, the model can learn to associate specific patterns in the input features with different medical conditions. This trained model can then be used to assist doctors in diagnosing patients by providing predictions and recommendations based on new patient data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd277d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696f00f",
   "metadata": {},
   "source": [
    "### 4. In supervised learning, what are classification and regression?\n",
    "\n",
    "Classification:\n",
    "Classification is a supervised learning task where the goal is to assign input data to predefined categories or classes. The algorithm learns from labeled training data and builds a model that can classify new, unseen instances into one of the predefined classes. In classification, the output variable is categorical, meaning it takes on discrete values or labels. Examples of classification tasks include spam email detection, sentiment analysis (classifying text as positive, negative, or neutral), and image recognition (identifying objects or faces in images).\n",
    "\n",
    "Regression:\n",
    "Regression is a supervised learning task that involves predicting a continuous numerical value or a quantity. The goal is to build a model that can learn the relationship between input features and the corresponding continuous output value. In regression, the output variable is numeric, and the algorithm aims to find a function that can map the input features to a numerical output. Examples of regression tasks include predicting housing prices based on features like square footage and number of bedrooms, forecasting stock prices based on historical data, and estimating patient health indicators like blood pressure based on various medical parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9a013",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a4e72",
   "metadata": {},
   "source": [
    "### 5. Give some popular classification algorithms as examples.\n",
    "\n",
    "1. Logistic regression\n",
    "2. Decision Trees\n",
    "3. Random Forest\n",
    "4. Support Vector Machines\n",
    "5. K-Nearest Neighbour (KNN)\n",
    "6. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a2f2d",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda6739",
   "metadata": {},
   "source": [
    "### 6. Briefly describe the SVM model.\n",
    "\n",
    "Support Vector Machines are powerful classification algorithms that aim to find an optimal hyperplane that separates data points of different classes with the largest margin. SVMs can handle linear and non-linear classification problems by using different kernel functions that map the data into higher-dimensional feature spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228af00",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dee584",
   "metadata": {},
   "source": [
    "### 7. In SVM, what is the cost of misclassification?\n",
    "\n",
    "In SVM (Support Vector Machines), the cost of misclassification, often referred to as the \"C\" parameter, determines the penalty for misclassifying data points. It is a hyperparameter that the user sets before training the SVM model.\n",
    "\n",
    "The cost of misclassification affects the trade-off between achieving a smaller margin and minimizing the misclassification of training examples. A smaller value of C allows for a larger margin but may lead to more misclassifications. Conversely, a larger value of C prioritizes the correct classification of training examples but may result in a smaller margin.\n",
    "\n",
    "Essentially, C controls the balance between the simplicity of the decision boundary (hyperplane) and the degree to which training examples can violate the margin or be misclassified. It helps control the SVM's bias-variance trade-off, where a low C may result in higher bias but lower variance, and a high C may result in lower bias but higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e7cc1",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957c35c",
   "metadata": {},
   "source": [
    "### 8. In the SVM model, define Support Vectors.\n",
    "\n",
    "Support vectors are the data points from the training set that lie closest to the decision boundary, or hyperplane, that separates different classes. These support vectors play a crucial role in defining the decision boundary and determining the SVM model's classification ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28ba25",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387691f",
   "metadata": {},
   "source": [
    "### 9. In the SVM model, define the kernel.\n",
    "\n",
    "A kernel is a function that is used to transform the input features into a higher-dimensional feature space. Kernels are employed to enable SVM to perform nonlinear classification tasks by implicitly mapping the original input data into a higher-dimensional space where a linear decision boundary can be more easily determined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675feb2e",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c78ac",
   "metadata": {},
   "source": [
    "### 10. What are the factors that influence SVM&#39;s effectiveness?\n",
    "\n",
    "1. Kernel selection\n",
    "2. Kernel Hyperparameters\n",
    "3. C parameter\n",
    "4. Data quality\n",
    "5. Data scaling\n",
    "6. Class imbalance\n",
    "7. Dimensionality\n",
    "8. Computational complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395da29",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24882f28",
   "metadata": {},
   "source": [
    "### 11. What are the benefits of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecdfa6",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a popular and powerful machine learning model with several benefits.\n",
    "\n",
    "1. Effective in high dimensional spaces\n",
    "2. Strong generalization\n",
    "3. Flexibility in kernel selection\n",
    "4. Robust against outliers\n",
    "5. Memory-efficient\n",
    "6. Ability to handle both linear and non-linear problems\n",
    "7. Interpretability\n",
    "8. Well studied research with tons of literature available to fine tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe05be",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45728bfb",
   "metadata": {},
   "source": [
    "### 12. What are the drawbacks of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef700d1e",
   "metadata": {},
   "source": [
    "1. Computationally expensive to work on large datasets.\n",
    "2. Hyperparameter tuning can be challenging especially kernel selection.\n",
    "3. Additionally, SVMs may not perform as well when the data is noisy or when the classes are heavily overlapped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13987c97",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49291aae",
   "metadata": {},
   "source": [
    "### 13. Notes should be written on\n",
    "\n",
    "1. The kNN algorithm has a validation flaw.\n",
    "\n",
    "The flaw in kNN arises when using traditional validation techniques such as cross-validation or train-test splits. In kNN, the algorithm makes predictions based on the majority vote or averaging of the labels of the k nearest neighbors. This means that the neighbors' labels have a significant influence on the prediction, and the choice of k becomes crucial.\n",
    "\n",
    "The problem with traditional validation approaches is that they assume independence between data points in the validation set, which may not hold true for kNN. If the validation set contains observations that are close to the training instances, it can result in overly optimistic performance estimates. In other words, the validation set may include neighbors of the training instances, leading to overly optimistic performance estimates because the model has already \"seen\" similar instances during training.\n",
    "\n",
    "2. In the kNN algorithm, the k value is chosen.\n",
    "\n",
    "The value of k is a crucial parameter that needs to be chosen. The value of k determines the number of nearest neighbors considered when making a prediction for a new data point.\n",
    "\n",
    "\n",
    "i. Bias-variance trade-off: A small value of k (e.g., 1) can lead to a low bias but high variance. This means that the model will be more sensitive to individual data points, potentially resulting in overfitting. On the other hand, a large value of k can lead to high bias but low variance, making the model less sensitive to individual data points but potentially oversimplifying the decision boundary.\n",
    "\n",
    "ii. Dataset characteristics: The optimal value of k may vary depending on the characteristics of the dataset. If the data has clear boundaries between classes, a smaller value of k might be appropriate. Conversely, if the data is noisy or contains overlapping classes, a larger value of k might help to smooth out the decision boundary.\n",
    "\n",
    "iii. Number of samples: The number of samples in the dataset can influence the choice of k. When the dataset is large, a smaller value of k might be sufficient as there is more data available to make accurate predictions. Conversely, in smaller datasets, a larger value of k can help reduce the impact of outliers or noise.\n",
    "\n",
    "iv. Computational considerations: The value of k can also impact the computational complexity of the algorithm. A larger value of k requires storing and comparing more neighbors, potentially increasing the algorithm's time and space complexity. Therefore, it's important to strike a balance between model performance and computational efficiency.\n",
    "\n",
    "3. A decision tree with inductive bias\n",
    "\n",
    "The concept of \"inductive bias\" refers to the set of assumptions or preferences that a learning algorithm uses to generalize from training data to unseen instances. In the context of decision trees, the algorithm's inductive bias influences the way it constructs and prunes the tree.\n",
    "\n",
    "Decision tree algorithms typically have an inherent inductive bias towards simplicity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff4b40",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6db44",
   "metadata": {},
   "source": [
    "### 14. What are some of the benefits of the kNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5961833",
   "metadata": {},
   "source": [
    "1. Simplicity and ease of implementation\n",
    "2. No training phase.\n",
    "3. Non-parametric and flexible.\n",
    "4. Adaptibility to new data.\n",
    "5. Robust to outliers.\n",
    "6. Interpretale results.\n",
    "7. Applicable to multi-class problems.\n",
    "8. No model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c4879",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac67d6f",
   "metadata": {},
   "source": [
    "### 15. What are some of the kNN algorithm&#39;s drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be311657",
   "metadata": {},
   "source": [
    "1. Computational Complexity where running large datasets.\n",
    "2. Storage Requirements for training datasets can be challenging.\n",
    "3. Sensitivity to Feature Scaling\n",
    "4. Optimal Value of k: Selecting the appropriate value for k is not always straightforward. A small value of k can lead to overfitting and sensitivity to noise, while a large value of k can result in oversmoothing and potentially miss important local patterns.\n",
    "5. kNN can be biased towards the majority class in imbalanced datasets. \n",
    "6. kNN can suffer from the curse of dimensionality, especially in high-dimensional feature spaces.\n",
    "7. The presence of noisy or irrelevant features can adversely affect the performance of kNN. \n",
    "8. In the prediction phase, kNN needs to calculate distances to all training instances for each new instance. This can be time-consuming, especially when dealing with large datasets or real-time applications where predictions need to be made quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc8f40",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405356eb",
   "metadata": {},
   "source": [
    "### 16. Explain the decision tree algorithm in a few words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971aecf1",
   "metadata": {},
   "source": [
    "The decision tree algorithm is a machine learning method that builds a tree-like model to make predictions based on a series of decision rules. It recursively partitions the data based on features and their conditions, aiming to create simple and interpretable decision boundaries. The tree structure facilitates classification and regression tasks by branching on different features, allowing for logical and sequential decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf22b",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7d799",
   "metadata": {},
   "source": [
    "### 17. What is the difference between a node and a leaf in a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fb47a",
   "metadata": {},
   "source": [
    "Nodes: Nodes in a decision tree represent decision points or splits. They contain conditions or rules based on features that determine the path of the tree. Nodes are responsible for dividing the data into subsets based on specific criteria. Each node represents a question or condition, and the data is partitioned into smaller subsets based on the answers to these questions.\n",
    "\n",
    "Leaves (or Terminal Nodes): Leaves, also known as terminal nodes, are the endpoints of a decision tree. They represent the final prediction or outcome of the tree for a given data instance. Leaves do not contain any further splits or conditions. Instead, they hold the predicted class label in the case of classification tasks or the predicted value in regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ffb9a",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8b587",
   "metadata": {},
   "source": [
    "### 18. What is a decision tree&#39;s entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a0405",
   "metadata": {},
   "source": [
    "Entropy, in the context of decision trees, is a measure of impurity or uncertainty associated with a given set of data. It quantifies the disorder or randomness in the distribution of class labels within the data.\n",
    "\n",
    "In decision tree algorithms, entropy is commonly used as a criterion to determine the quality of a split at each node. The goal is to find the splits that minimize entropy and result in more homogeneous subsets of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b246a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc636e",
   "metadata": {},
   "source": [
    "### 19. In a decision tree, define knowledge gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267809dd",
   "metadata": {},
   "source": [
    "In a decision tree, knowledge gain, also known as information gain, is a metric used to evaluate the quality of a potential split at a node. It measures the reduction in entropy or impurity achieved by splitting the data based on a specific feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990ed20",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce940c7",
   "metadata": {},
   "source": [
    "### 20. Choose three advantages of the decision tree approach and write them down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef8780",
   "metadata": {},
   "source": [
    "1. Decision trees offer transparent and interpretable models. The structure of the tree, with its sequence of decision rules, provides a clear understanding of how the model makes predictions. Each split and node in the tree corresponds to a feature and condition, allowing users to interpret the decision-making process and gain insights into the underlying data.\n",
    "\n",
    "2. Decision trees can effectively handle non-linear relationships between features and the target variable. The recursive partitioning process allows the algorithm to capture complex decision boundaries by combining multiple splits and conditions. This flexibility enables decision trees to represent and learn non-linear patterns in the data without relying on explicit assumptions about the data distribution.\n",
    "\n",
    "3. Decision trees can handle both categorical and numerical features without requiring explicit feature scaling or transformation. The algorithm can naturally handle a mix of feature types in the same dataset. It determines the best splitting criteria based on the nature of the features and selects the most informative conditions for the respective feature types. This characteristic simplifies the preprocessing steps and makes decision trees applicable to a wide range of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cf29e",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f07620",
   "metadata": {},
   "source": [
    "### 21. Make a list of three flaws in the decision tree process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16db69b",
   "metadata": {},
   "source": [
    "1. Decision trees are prone to overfitting, especially when the tree becomes overly complex. If the tree is allowed to grow too deep or if it is not pruned appropriately, it can capture noise or outliers in the training data, leading to poor generalization on unseen instances. Overfitting can result in overly specific decision rules that do not generalize well beyond the training set.\n",
    "2. Decision trees can be sensitive to small changes in the training data. A slight variation in the dataset or the addition/removal of a few instances can lead to different splits and decision boundaries. This instability makes decision trees less robust compared to some other machine learning algorithms. \n",
    "3. In decision trees, features with more levels or categories tend to have a higher chance of being selected for splitting. This bias arises because the algorithm uses heuristics such as information gain or Gini index, which favor features that provide more opportunities for splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc156d94",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d256c",
   "metadata": {},
   "source": [
    "### 22. Briefly describe the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d21711",
   "metadata": {},
   "source": [
    "The random forest model is an ensemble learning method that combines multiple decision trees to make predictions. It is a popular and powerful machine learning algorithm that offers improved accuracy and robustness compared to individual decision trees.\n",
    "\n",
    "In a random forest, multiple decision trees are trained independently on random subsets of the training data, known as bootstrap samples. Additionally, at each split in each tree, only a random subset of features is considered for splitting. This combination of random sampling and feature selection introduces variation and reduces the correlation between the individual trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
