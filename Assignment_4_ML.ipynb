{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cda89e1",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks involved in getting ready to work with machine learning modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3333",
   "metadata": {},
   "source": [
    "When preparing to work with machine learning modeling, there are several key tasks that you should consider:\n",
    "\n",
    "Define the problem: Clearly articulate the problem you want to solve or the goal you want to achieve with machine learning. Understand the requirements, constraints, and expected outcomes of the project.\n",
    "\n",
    "Gather and understand the data: Collect relevant data for your problem domain. This may involve data acquisition from various sources, such as databases, APIs, or external datasets. Thoroughly explore and analyze the data to gain insights, understand the quality, and identify any data preprocessing steps that may be necessary.\n",
    "\n",
    "Data preprocessing: Clean the data and handle any missing, incorrect, or inconsistent values. This may involve techniques like data imputation, removing duplicates, dealing with outliers, and handling categorical variables through encoding or one-hot encoding. Scaling or normalizing features may also be required.\n",
    "\n",
    "Feature selection and engineering: Identify the most informative features that will be used as inputs for your machine learning model. Consider the relevance, redundancy, and importance of features. You may also need to create new features or transform existing ones to enhance their predictive power.\n",
    "\n",
    "Split the data: Divide your dataset into training, validation, and test sets. The training set is used to train the model, the validation set helps optimize model parameters and make adjustments, and the test set is used for final evaluation to assess the generalization of the model.\n",
    "\n",
    "Select a suitable algorithm: Choose an appropriate machine learning algorithm based on the problem type, data characteristics, and available resources. Consider algorithms such as linear regression, decision trees, support vector machines, neural networks, or ensemble methods.\n",
    "\n",
    "Model training and evaluation: Train the selected model on the training data. Tune the hyperparameters of the model to optimize its performance using techniques like grid search or random search. Evaluate the model's performance using appropriate metrics and validation techniques such as cross-validation.\n",
    "\n",
    "Model validation and tuning: Validate the trained model using the validation dataset to assess its generalization and performance on unseen data. Fine-tune the model, adjust hyperparameters, or consider different algorithms based on the validation results. Iterate this process until satisfactory performance is achieved.\n",
    "\n",
    "Final model evaluation: Once you have fine-tuned your model, evaluate its performance on the test dataset. This gives you an unbiased estimate of how well the model is expected to perform in real-world scenarios.\n",
    "\n",
    "Deployment and monitoring: Deploy the trained model into a production environment to make predictions on new, unseen data. Monitor the model's performance over time and consider retraining or updating the model as new data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317ebed",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fd04c",
   "metadata": {},
   "source": [
    "### 2. What are the different forms of data used in machine learning? Give a specific example for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08568780",
   "metadata": {},
   "source": [
    "In machine learning, data can come in different forms, each with its own characteristics and considerations. Here are some common forms of data used in machine learning, along with specific examples:\n",
    "\n",
    "1. Numerical Data:\n",
    "Numerical data consists of quantitative values that can be represented by numbers. Examples include temperature readings, stock prices, or a person's age. This type of data is suitable for regression tasks, where the goal is to predict a continuous value.\n",
    "\n",
    "2. Categorical Data:\n",
    "Categorical data represents discrete values that belong to specific categories or labels. Examples include the type of car (sedan, SUV, truck), the color of a shirt (red, blue, green), or a person's gender (male, female). Categorical data is often encoded using one-hot encoding or label encoding before being used in machine learning models. It is commonly used for classification tasks.\n",
    "\n",
    "3. Textual Data:\n",
    "Textual data consists of free-form text, such as customer reviews, social media posts, or news articles. Natural Language Processing (NLP) techniques are applied to process and extract meaningful information from text data. Examples of NLP tasks include sentiment analysis, text classification, or language translation.\n",
    "\n",
    "4. Image Data:\n",
    "Image data consists of visual information in the form of pixels, such as photographs or digital images. Each pixel carries color and intensity information. Image data is commonly represented as arrays or tensors of pixel values. Applications of image data in machine learning include object detection, image classification, and image generation.\n",
    "\n",
    "5. Time Series Data:\n",
    "Time series data is collected over a sequence of time points, with measurements taken at regular intervals. Examples include stock market prices, temperature records, or sensor data. Time series analysis techniques, such as autoregressive models or recurrent neural networks, are used to capture patterns and make predictions based on the temporal aspect of the data.\n",
    "\n",
    "6. Audio Data:\n",
    "Audio data represents sounds or speech signals captured in the form of waveforms. Examples include music recordings, voice commands, or audio from sensor devices. Techniques like Fourier transforms or spectrogram analysis are employed to extract features from audio data. Audio data is used in applications such as speech recognition, music classification, or speaker identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c5c5f",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0dfb5",
   "metadata": {},
   "source": [
    "### 3. Distinguish:\n",
    "\n",
    "1. Numeric vs. categorical attributes\n",
    "\n",
    "2. Feature selection vs. dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2c1f1",
   "metadata": {},
   "source": [
    "Numerical and categorical attributes are two different types of data attributes used in various fields, including statistics, data analysis, and machine learning. Here's how you can distinguish between them:\n",
    "\n",
    "Numerical Attributes:\n",
    "\n",
    "Numerical attributes represent quantitative measurements or quantities.\n",
    "They can take on a wide range of numerical values, such as integers or real numbers.\n",
    "Numerical attributes are typically continuous or discrete. Continuous attributes can take any value within a range, while discrete attributes can only take specific values.\n",
    "Examples of numerical attributes include age, weight, height, temperature, and income.\n",
    "Numerical attributes can be further classified as interval or ratio attributes. Interval attributes have a consistent measurement scale but lack a meaningful zero point, while ratio attributes have both a consistent scale and a meaningful zero point.\n",
    "Categorical Attributes:\n",
    "\n",
    "Categorical attributes represent qualitative or descriptive characteristics.\n",
    "They take on a limited number of distinct values or categories.\n",
    "Categorical attributes can be nominal or ordinal. Nominal attributes have categories without any inherent order, while ordinal attributes have categories with a specific order or ranking.\n",
    "Examples of categorical attributes include gender (male, female), color (red, blue, green), marital status (single, married, divorced), and education level (high school, bachelor's, master's, Ph.D.).\n",
    "Categorical attributes are often represented using labels or codes, but the actual values do not hold any numerical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d813af",
   "metadata": {},
   "source": [
    "Feature selection and dimensionality reduction are techniques used in data preprocessing and feature engineering to reduce the number of input variables or features in a dataset. Although they serve a similar purpose, there are differences between the two:\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Feature selection refers to the process of selecting a subset of relevant features from the original feature set.\n",
    "The goal of feature selection is to improve model performance by reducing the complexity and noise in the dataset.\n",
    "Feature selection methods evaluate the relevance and importance of each feature and select a subset based on certain criteria.\n",
    "Selected features are retained, and the rest are discarded, resulting in a reduced feature space.\n",
    "Feature selection can be performed using various techniques such as filter methods (e.g., correlation, information gain), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., LASSO regression).\n",
    "Feature selection can help improve model interpretability, reduce training time, and prevent overfitting.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Dimensionality reduction aims to transform the original high-dimensional feature space into a lower-dimensional space while preserving the essential structure and information of the data.\n",
    "It involves creating new features or dimensions that are a combination of the original features.\n",
    "Dimensionality reduction methods aim to capture the most important information in the data while minimizing information loss.\n",
    "Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are commonly used dimensionality reduction techniques.\n",
    "In dimensionality reduction, all original features are used in the transformation process, and the resulting reduced feature space typically has fewer dimensions than the original space.\n",
    "Dimensionality reduction can help mitigate the curse of dimensionality, improve computational efficiency, and aid in visualization.\n",
    "In summary, feature selection focuses on identifying and retaining the most relevant features from the original set, while dimensionality reduction involves transforming the entire set of features into a lower-dimensional space. Both techniques aim to simplify the dataset, but their approaches and outcomes differ. The choice between feature selection and dimensionality reduction depends on the specific requirements of the problem, the characteristics of the dataset, and the intended use of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da9bbb",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b31f69",
   "metadata": {},
   "source": [
    "### 4. Make quick notes on any two of the following:\n",
    "\n",
    "1. The histogram - A histogram is a graphical representation of data that uses bars to display the frequency distribution of a dataset. It provides a visual summary of the underlying data and helps in understanding the distribution of values within a dataset.\n",
    "\n",
    "2. Use a scatter plot - A scatter plot is a type of data visualization that displays the relationship between two variables. It uses a Cartesian coordinate system where each data point is represented by a dot or marker on the plot. The position of the dot on the plot corresponds to the values of the two variables being compared.\n",
    "\n",
    "3. PCA (Personal Computer Aid) - A personal computer aid refers to any tool, software, or feature that assists an individual in using their personal computer more effectively and efficiently. Personal computer aids can take various forms and serve different purposes, depending on the needs and preferences of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319fa2a6",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034766c9",
   "metadata": {},
   "source": [
    "### 5. Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative data are explored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301660e",
   "metadata": {},
   "source": [
    "### Investigating data is crucial for gaining insights, making informed decisions, and drawing meaningful conclusions. Whether dealing with qualitative or quantitative data, the investigation process helps uncover patterns, relationships, trends, and anomalies, leading to a deeper understanding of the underlying phenomenon.\n",
    "### While the objectives and techniques of qualitative and quantitative data may differ, both qualitative and quantitative data investigations share common goals of extracting insights and generating knowledge. In practice, researchers may use a mixed-methods approach, combining qualitative and quantitative data, to obtain a more comprehensive understanding of a research problem. This allows for triangulation, where different data sources and analysis methods are used to validate and complement each other's findings.\n",
    "\n",
    "### In summary, investigating data, whether qualitative or quantitative, is necessary to extract meaningful information and generate insights. The specific techniques employed may vary based on the nature of the data, but the overarching goal remains the same: to gain a deeper understanding of the research question or phenomenon under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1092cc5",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b80568",
   "metadata": {},
   "source": [
    "### 6. What are the various histogram shapes? What exactly are ‘bins&#39;?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf30a2",
   "metadata": {},
   "source": [
    "Histograms can exhibit different shapes depending on the distribution of the data. Here are some common shapes of histograms:\n",
    "\n",
    "1. Normal Distribution (Bell-shaped)\n",
    "2. Skewed right\n",
    "3. Skewed left\n",
    "4. Bimodal\n",
    "5. Uniform\n",
    "6. U-shaped\n",
    "7. Rectangular\n",
    "8. Multimodal\n",
    "\n",
    "- To create a histogram, the first step is to divide the entire range of the data into a series of intervals or bins. The width of these bins can be determined based on the range of values in the dataset and the desired level of detail. Each bin represents a specific range of values, and the height of the bar represents the frequency or count of data points falling within that bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81f3ec",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e2c7a",
   "metadata": {},
   "source": [
    "### 7. How do we deal with data outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44264252",
   "metadata": {},
   "source": [
    "### Start by identifying the outliers in your dataset. You can use statistical methods such as z-scores, modified z-scores, or the interquartile range (IQR) to detect values that fall significantly outside the expected range.\n",
    "\n",
    "### 1. Univariate Method: This method looks for data points with extreme values on one variable.\n",
    "\n",
    "### 2. Multivariate Method: Here, we look for unusual combinations of all the variables.\n",
    "\n",
    "### 3. Minkowski Error: This method reduces the contribution of potential outliers in the training process.\n",
    "\n",
    "### 4. Z-Score: z = (x - μ) / σ\n",
    "\n",
    "- where:\n",
    "\n",
    "z is the z-score\n",
    "x is the data point you want to calculate the z-score for\n",
    "μ is the mean of the data set\n",
    "σ is the standard deviation of the data set\n",
    "\n",
    "### 5. IQR Score: Calculate IQR score to filter out the outliers by keeping only valid values.boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 *IQR))).any(axis=1)] boston_df_out.shape\n",
    "\n",
    "### 6. Quantile function: Use quantile() to remove amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a556ab",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7af9b",
   "metadata": {},
   "source": [
    "### 8. What are the various central inclination measures? Why does mean vary too much from median in certain data sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f718",
   "metadata": {},
   "source": [
    "### The central inclination measures, also known as measures of central tendency, are statistical measures that provide information about the center or typical value of a dataset. The three commonly used central inclination measures are the mean, median, and mode.\n",
    "### The mean and median can vary significantly in certain datasets due to the presence of outliers or skewed distributions. When a dataset has extreme values, the mean can be heavily influenced because it considers every value, including the outliers. On the other hand, the median is more resistant to outliers because it only takes into account the middle value(s) and ignores the specific magnitude of the outliers. Consequently, the mean can be pulled towards the extreme values, causing it to deviate from the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd6ecd",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf82f34",
   "metadata": {},
   "source": [
    "### 9. Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find outliers using a scatter plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491e2dd",
   "metadata": {},
   "source": [
    "### A scatter plot is a type of graphical representation that displays the relationship between two variables. It is useful for investigating bivariate relationships, which involve analyzing the association or correlation between two sets of data points.\n",
    "\n",
    "### While scatter plots provide a visual means to detect outliers, it is important to note that they may not provide a definitive determination of whether a data point is truly an outlier or if it carries meaningful information. Further analysis and statistical techniques may be required to confirm and understand the nature of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4a4d1",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4379d",
   "metadata": {},
   "source": [
    "### 10. Describe how cross-tabs can be used to figure out how two variables are related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4fa6b",
   "metadata": {},
   "source": [
    "### Cross tabs are particularly useful for analyzing categorical data and understanding the relationship between two variables that have distinct categories or levels. They provide a structured and visual representation of the data, allowing for a comprehensive examination of the relationship and facilitating further analysis and interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
