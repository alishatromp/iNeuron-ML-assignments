{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399533f6",
   "metadata": {},
   "source": [
    "### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d76c4",
   "metadata": {},
   "source": [
    "Main reasons: \n",
    "\n",
    "1. Dimensionality reduction helps overcome the curse of dimensionality by reducing the number of features and focusing on the most informative ones.\n",
    "2. Improves computational efficiency.\n",
    "3. Overfitting prevention.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. By reducing the dimensions, there is a possibility of losing some information present in the original dataset.\n",
    "2. It can also make the data less interpretable.\n",
    "3. Dimensionality reduction algorithms themselves can be complex and computationally demanding. Techniques like Principal Component Analysis (PCA) or t-SNE require additional computation time and resources, especially for large datasets.\n",
    "4. Parameter tuning.\n",
    "5. Pertains to the charachter type of data. May work for some types and not for some."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bb28e",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635d306",
   "metadata": {},
   "source": [
    "### 2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c37aab",
   "metadata": {},
   "source": [
    "As the number of dimensions (features) increases, the amount of data required to generalize accurately grows exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ed5b7",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc210",
   "metadata": {},
   "source": [
    "### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8f1c2",
   "metadata": {},
   "source": [
    "It is not possibly to perfectly reverse the process of reducing dimensionality of a dataset. DR eliminates features and during this process, some information is inevitably lost.\n",
    "\n",
    "If you want to recover a higher-dimensional representation from a lower-dimensional representation obtained through dimensionality reduction, you can employ techniques like dimensionality expansion or feature reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d9cb",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792fb26",
   "metadata": {},
   "source": [
    "### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6e1d8",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is primarily designed for linear dimensionality reduction, meaning it works best when the underlying relationships between variables are linear. If the dataset contains nonlinear relationships, PCA may not capture the nonlinear structure effectively. In such cases, alternative nonlinear dimensionality reduction techniques can be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71046dc4",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2e401",
   "metadata": {},
   "source": [
    "### 5. Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d67af9",
   "metadata": {},
   "source": [
    "If PCA on a 1,000-dimensional dataset has a 95 percent explained variance ratio, it means that the resulting dataset retains 95 percent of the total variance in the original dataset. Therefore number of dimensions may vary depending on the dataset and the specific distribution of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87531fe2",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb384b09",
   "metadata": {},
   "source": [
    "### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657b085",
   "metadata": {},
   "source": [
    "The choice of PCA variant depends on the characteristics of the dataset and the specific requirements of the analysis.\n",
    "\n",
    "1. Vanilla PCA refers to the standard implementation of PCA. It is suitable for datasets that can fit into memory, and when all the data is available at once. Vanilla PCA calculates the covariance matrix of the entire dataset and performs eigendecomposition to extract the principal components. It is a good choice when memory is not a constraint and the entire dataset can be loaded into memory.\n",
    "\n",
    "2. Incremental PCA (IPCA) is used when memory limitations prevent loading the entire dataset at once. IPCA processes the data in batches or chunks, updating the covariance matrix incrementally.\n",
    "\n",
    "3. Randomized PCA is an approximation algorithm that provides a fast and memory-efficient alternative to standard PCA. It uses randomized techniques to compute an approximation of the principal components.\n",
    "\n",
    "4. Kernel PCA is suitable for datasets that exhibit nonlinear relationships. It employs the kernel trick to implicitly transform the data into a higher-dimensional feature space where linear PCA can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e4115",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698636fe",
   "metadata": {},
   "source": [
    "### 7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f2c41",
   "metadata": {},
   "source": [
    "1. One way to assess the algorithm's success is by visualizing the data in the reduced dimensional space. Plot the data points and observe if the reduced representation captures the inherent structure or patterns in the data. \n",
    "\n",
    "2. For algorithms that support reconstruction, such as PCA, calculate the reconstruction error or loss. It quantifies the discrepancy between the original data and the reconstructed data in the higher-dimensional space. Lower reconstruction errors indicate better preservation of information during the dimensionality reduction process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30489bd",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af8984",
   "metadata": {},
   "source": [
    "### 8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d0575",
   "metadata": {},
   "source": [
    "Yes, it can be logical to use two different dimensionality reduction algorithms in a chain. \n",
    "\n",
    "1. You start with a high-dimensional dataset and apply a first dimensionality reduction algorithm, such as PCA or Randomized PCA, to reduce the initial dimensionality. \n",
    "2. After the first reduction, you may still have a remaining dataset with some nonlinear relationships or intricate structures. In this case, you can apply a second dimensionality reduction algorithm, such as Kernel PCA or t-SNE, to capture nonlinear patterns or preserve complex manifold structures that the first reduction may not have fully captured."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
